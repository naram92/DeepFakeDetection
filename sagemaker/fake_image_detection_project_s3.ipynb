{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Yly4fQhA-LO"
      },
      "source": [
        "# DEEPFAKE DETECTION PROJECT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "tWguqkOXA-Lz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "from pathlib import Path\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from functools import partial\n",
        "\n",
        "from blazeface import BlazeFace\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "from poutyne.framework import Model, ModelCheckpoint, BestModelRestore, CSVLogger\n",
        "import boto3\n",
        "import io\n",
        "import botocore\n",
        "import tempfile\n",
        "from pathlib import PurePosixPath\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "s3_resource = boto3.resource('s3')\n",
        "s3_client = boto3.client('s3')\n",
        "bucket_name = 'deepfake-detection'\n",
        "bucket = s3_resource.Bucket(bucket_name)\n",
        "\n",
        "# use m1 cuda accelaration \n",
        "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6rFYG20A-L9"
      },
      "source": [
        "## Datasets Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8zrO7GjUA-L_"
      },
      "outputs": [],
      "source": [
        "FFPP_SRC = 'datasets/'\n",
        "VIDEODF_SRC = os.path.join(FFPP_SRC, 'ffpp_videos.pkl')\n",
        "\n",
        "BLAZEFACE_WEIGHTS = \"blazeface/blazeface.pth\"\n",
        "BLAZEFACE_ANCHORS = \"blazeface/anchors.npy\"\n",
        "\n",
        "FACES_DST = os.path.join(FFPP_SRC, 'extract_faces')\n",
        "FACESDF_DST = os.path.join(FACES_DST, 'ffpp_faces.pkl')\n",
        "CHECKPOINT_DST = os.path.join(FACES_DST, 'checkpoint')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkMswaAHA-MF"
      },
      "source": [
        "### FF++ Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_ffpp(source_dir, video_dataset_path):\n",
        "    \"\"\"\n",
        "    Preprocessing video dataset : Set the label of each video {0 for real video, \n",
        "    1 for fake video} and the video original of fake videos.\n",
        "    :param source_dir: the parent directory that contains all videos (real or \n",
        "                        fake)\n",
        "    :param video_dataset_path: Path to save the videos DataFrame[path, name, \n",
        "                                label, original]\n",
        "    \"\"\" \n",
        "    try:\n",
        "        s3_resource.Object(bucket_name, video_dataset_path).load()\n",
        "        file_exists = True\n",
        "    except botocore.exceptions.ClientError as e:\n",
        "        if e.response['Error']['Code'] == \"404\":\n",
        "            file_exists = False\n",
        "        else:\n",
        "            raise\n",
        "\n",
        "    # Charger le fichier de checkpoint depuis S3 s'il existe\n",
        "    if file_exists:\n",
        "        df_videos = pickle.loads(s3_resource.Bucket(bucket_name).Object(video_dataset_path).get()['Body'].read())\n",
        "    else :\n",
        "        # Si le fichier n'existe pas dans S3, vous pouvez créer le DataFrame de la façon suivante :\n",
        "        print('Creating video DataFrame')\n",
        "        # df_videos = pd.DataFrame(columns=['path', 'label', 'name', 'original'])\n",
        "\n",
        "        # Initialisez une liste vide pour stocker les chemins des fichiers .mp4\n",
        "        mp4_files = []\n",
        "\n",
        "        # Appelez la méthode list_objects_v2 de l'objet client S3 en spécifiant le paramètre ContinuationToken\n",
        "        # lors de chaque itération, jusqu'à ce qu'il n'y ait plus d'objets à récupérer \n",
        "        objects_list = s3_client.list_objects_v2(\n",
        "            Bucket=bucket_name, Prefix=source_dir\n",
        "        )\n",
        "        while True:\n",
        "            # Parcourez la liste des objets et ajoutez le chemin de chaque fichier mp4 à la liste\n",
        "            for obj in objects_list['Contents']:\n",
        "                # Vérifiez si l'objet est un fichier mp4\n",
        "                if obj['Key'].endswith('.mp4'):\n",
        "                    # Ajoutez le chemin du fichier à la liste\n",
        "                    mp4_files.append(obj['Key'])\n",
        "            \n",
        "            # Vérifiez si il y a une suite de résultats\n",
        "            if 'NextContinuationToken' in objects_list:\n",
        "                # Si oui, récupérez la suite des résultats en spécifiant le ContinuationToken\n",
        "                objects_list = s3_client.list_objects_v2(\n",
        "                    Bucket=bucket_name, Prefix=source_dir, ContinuationToken=objects_list['NextContinuationToken']\n",
        "                )\n",
        "            else:\n",
        "                # Si non, sortez de la boucle\n",
        "                break\n",
        "        \n",
        "        # Créez le DataFrame en utilisant la liste des chemins de fichiers .mp4\n",
        "        df_videos = pd.DataFrame({'path': mp4_files})\n",
        "        # Enlevez le repertoire racine datasets/ dans le path\n",
        "        df_videos['path'] = df_videos['path'].replace('datasets/', '', regex=True)\n",
        "        # Convertissez les chaînes de caractères en objets PurePosixPath\n",
        "        df_videos['path'] = df_videos['path'].apply(lambda x: PurePosixPath(x))           \n",
        "\n",
        "        # 1 if fake, otherwise 0\n",
        "        df_videos['label'] = df_videos['path'].map(\n",
        "            lambda x: 1 if x.parts[0] == 'manipulated_sequences' else 0)\n",
        "        \n",
        "        source = df_videos['path'].map(lambda x: x.parts[1]).astype('category')\n",
        "        df_videos['name'] = df_videos['path'].map(lambda x: x.with_suffix('').parts[-1])\n",
        "        df_videos['path'] = df_videos['path'].map(lambda x: str(x))\n",
        "\n",
        "        df_videos['original'] = -1 * np.ones(len(df_videos), dtype=np.int16)\n",
        "        # Mettre dans la colonne original l'index de l'original des fakes\n",
        "        df_videos.loc[(df_videos['label'] == 1) & (source != 'DeepFakeDetection'), 'original'] = \\\n",
        "            df_videos[(df_videos['label'] == 1) & (source != 'DeepFakeDetection')]['name'].map(\n",
        "                lambda x: df_videos.index[np.flatnonzero(df_videos['name'] == x.split('_')[0])[0]]\n",
        "            )\n",
        "        df_videos.loc[(df_videos['label'] == 1) & (source == 'DeepFakeDetection'), 'original'] = \\\n",
        "            df_videos[(df_videos['label'] == 1) & (source == 'DeepFakeDetection')]['name'].map(\n",
        "                lambda x: df_videos.index[\n",
        "                    np.flatnonzero(df_videos['name'] == x.split('_')[0] + '__' + x.split('__')[1])[0]]\n",
        "            )\n",
        "    \n",
        "        # Enregistrez le DataFrame dans S3\n",
        "        print('Saving video DataFrame to {}'.format(video_dataset_path))\n",
        "        buf = io.BytesIO()\n",
        "        df_videos.to_pickle(buf)\n",
        "        buf.seek(0)\n",
        "        bucket.upload_fileobj(buf, video_dataset_path)\n",
        "    \n",
        "    print('Real videos: {:d}'.format(sum(df_videos['label'] == 0)))\n",
        "    print('Fake videos: {:d}'.format(sum(df_videos['label'] == 1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "id": "50I8DTgAA-MX",
        "outputId": "50299075-5427-4974-ffbd-15c48da73d86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Real videos: 1000\n",
            "Fake videos: 2000\n"
          ]
        }
      ],
      "source": [
        "# preprocess ff++ data\n",
        "preprocess_ffpp(FFPP_SRC, VIDEODF_SRC)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YjT22kUA-MN"
      },
      "source": [
        "### Extract faces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_faces_on_video(video_df, source_dir, faces_dir, checkpoint_dir, \n",
        "                           blazeface, num_frames, face_size=224, margin=0.25):\n",
        "    \"\"\"\n",
        "    This function extracts `num_frames` frames in the videos that contain a face.\n",
        "    :param video_df: the DataFrame that contains all informations about the \n",
        "                    datasets. It has the following columns: [path, name, \n",
        "                    label, original].\n",
        "    :param source_dir: the parent directory that contains the datasets\n",
        "    :param faces_dir: the directory path to save the extracted faces from the \n",
        "                    datasets\n",
        "    :param checkpoint_dir: the directory path to save the DataFrame[path, label,\n",
        "                    video, original, frame_index, score, detection] of the \n",
        "                    extracted faces\n",
        "    :param blazeface: a Balazeface object that will be used as face detector in\n",
        "                    all frames\n",
        "    :param num_frames: number of frames to extract in each video.\n",
        "    :param face_size (default = 224) : each frame extracted will have the size\n",
        "                    face_size x face_size\n",
        "    :param margin (default = 0.25) : Offset margin of face detection.\n",
        "    \"\"\"\n",
        "    video_idx, video_df = video_df\n",
        "    faces_checkpoint_path = Path(checkpoint_dir).joinpath(video_df['path'].split('.')[0] + '_faces.pkl')\n",
        "    \n",
        "    try:\n",
        "        s3_resource.Object(bucket_name, str(faces_checkpoint_path)).load()\n",
        "        file_exists = True\n",
        "    except botocore.exceptions.ClientError as e:\n",
        "        if e.response['Error']['Code'] == \"404\":\n",
        "            file_exists = False\n",
        "        else:\n",
        "            raise\n",
        "        \n",
        "    if file_exists: \n",
        "        faces = pickle.loads(s3_resource.Bucket(bucket_name).Object(str(faces_checkpoint_path)).get()['Body'].read())\n",
        "        return faces\n",
        "        \n",
        "    else :\n",
        "        # Télécharger la vidéo depuis S3 dans un buffer\n",
        "        video_path = Path(source_dir).joinpath(video_df['path'])\n",
        "        url = s3_client.generate_presigned_url(ClientMethod='get_object', Params={ 'Bucket': bucket_name, 'Key': str(video_path) })\n",
        "        reader =  cv2.VideoCapture(url)\n",
        "        # Obtenir le nombre de frames de la vidéo\n",
        "        frame_count = int(reader.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        # Obtenir un tableau d'indices de frames uniformément répartis dans la vidéo\n",
        "        frame_idx = np.unique(np.linspace(0, frame_count - 1, num_frames, dtype=int))\n",
        "        # Get the frames choosen\n",
        "        frames, idx = [], 0\n",
        "        # Tant que la vidéo peut être lue\n",
        "        while reader.grab():\n",
        "            if idx in frame_idx:\n",
        "                ret, frame = reader.retrieve()\n",
        "                if not ret or frame is None:\n",
        "                    print(\"Error retrieving frame %d from movie %s\" % (frame_idx, path))\n",
        "                    break\n",
        "                frames.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "            idx += 1\n",
        "        frames = np.stack(frames) # Empiler les frames dans un tableau NumPy\n",
        "        \n",
        "        # Obtenir la largeur et la hauteur cibles pour le modèle BlazeFace\n",
        "        target_w, target_h = blazeface.input_size\n",
        "        # Pour chaque frame, obtenir plusieurs tuiles de taille target_w x target_h\n",
        "        num_frames, height, width, _ = frames.shape\n",
        "            # Définir la taille de chaque tuile en prenant la plus petite valeur parmi height, width et 720\n",
        "        split_size = min(height, width, 720)\n",
        "        x_step = (width - split_size) // 2\n",
        "        y_step = (height - split_size) // 2\n",
        "        num_h = (height - split_size) // y_step + 1 if y_step > 0 else 1\n",
        "        num_w = (width - split_size) // x_step + 1 if x_step > 0 else 1\n",
        "\n",
        "        tiles = np.zeros((num_frames * num_h * num_w, target_h, target_w, 3), \n",
        "                        dtype=np.uint8)\n",
        "        i = 0\n",
        "        for f in range(num_frames):\n",
        "            y = 0\n",
        "            for _ in range(num_h):\n",
        "                x = 0\n",
        "                for __ in range(num_w):\n",
        "                    # Découper une tuile à partir de la frame actuelle\n",
        "                    crop = frames[f, y:y + split_size, x:x + split_size, :]\n",
        "                    # Redimensionner la tuile à la taille cible en utilisant une interpolation par aire\n",
        "                    tiles[i] = cv2.resize(crop, (target_w, target_h), \n",
        "                                        interpolation=cv2.INTER_AREA)\n",
        "                    x += x_step\n",
        "                    i += 1\n",
        "                y += y_step\n",
        "\n",
        "        # Run the face detector. The result is a list of PyTorch tensors\n",
        "        detections = blazeface.predict_on_batch(tiles, apply_nms=False)\n",
        "        # Convert the detections from 128x128 back to the original frame size\n",
        "        for i in range(len(detections)):\n",
        "            # ymin, xmin, ymax, xmax\n",
        "            for k in range(2):\n",
        "                detections[i][:, k * 2] = (detections[i][:, k * 2] * target_h) * split_size / target_h\n",
        "                detections[i][:, k * 2 + 1] = (detections[i][:, k * 2 + 1] * target_w) * split_size / target_w\n",
        "\n",
        "        # Because we have several tiles for each frame, combine the predictions from these tiles.\n",
        "        combined_detections = []\n",
        "        i = 0\n",
        "        for f in range(num_frames):\n",
        "            detections_for_frame = []\n",
        "            y = 0\n",
        "            for _ in range(num_h):\n",
        "                x = 0\n",
        "                for __ in range(num_w):\n",
        "                    # Adjust the coordinates based on the split positions.\n",
        "                    if detections[i].shape[0] > 0:\n",
        "                        for k in range(2):\n",
        "                            detections[i][:, k * 2] += y\n",
        "                            detections[i][:, k * 2 + 1] += x\n",
        "                    \n",
        "                    detections_for_frame.append(detections[i])\n",
        "                    x += x_step\n",
        "                    i += 1\n",
        "                y += y_step\n",
        "                \n",
        "            combined_detections.append(torch.cat(detections_for_frame))\n",
        "        if len(combined_detections) == 0:\n",
        "            return None\n",
        "        detections = blazeface.nms(combined_detections)\n",
        "        # Crop the faces out of the original frame.\n",
        "        faces = []\n",
        "        for i in range(len(detections)):\n",
        "            offset = torch.round(margin * (detections[i][:, 2] - detections[i][:, 0])) # margin 0.2\n",
        "            detections[i][:, 0] = torch.clamp(detections[i][:, 0] - offset * 2, min=0)  # ymin\n",
        "            detections[i][:, 1] = torch.clamp(detections[i][:, 1] - offset, min=0)  # xmin\n",
        "            detections[i][:, 2] = torch.clamp(detections[i][:, 2] + offset, max=height)  # ymax\n",
        "            detections[i][:, 3] = torch.clamp(detections[i][:, 3] + offset, max=width)  # xmax\n",
        "            \n",
        "            # Get the first best scored face\n",
        "            score, face, detection = 0, None, None\n",
        "            for j in range(len(detections[i])):\n",
        "                if score < detections[i][j][16].cpu():\n",
        "                    detection = detections[i][j].cpu()\n",
        "                    ymin, xmin, ymax, xmax = detection[:4].cpu().numpy().astype(int)\n",
        "                    face = frames[i][ymin:ymax, xmin:xmax, :]\n",
        "                    score = detection[16]\n",
        "                    break\n",
        "            if face is not None:\n",
        "                image = Image.fromarray(face)\n",
        "                # Crop the image to face_size x face_size\n",
        "                top, left, bottom, right = detection[:4].cpu().numpy().astype(int)\n",
        "                x_ctr = (left + right) // 2\n",
        "                y_ctr = (top + bottom) // 2\n",
        "                new_top = max(y_ctr - face_size // 2, 0)\n",
        "                new_bottom = min(new_top + face_size, height)\n",
        "                new_left = max(x_ctr - face_size // 2, 0)\n",
        "                new_right = min(new_left + face_size, width)\n",
        "                image.crop([new_left, new_top, new_right, new_bottom])\n",
        "                # Save image\n",
        "                face_path = Path(faces_dir).joinpath(video_df['path']).joinpath('frame_{}.jpg'.format(frame_idx[i]))\n",
        "                buf = io.BytesIO()\n",
        "                image.save(buf, format=\"jpeg\")\n",
        "                object = bucket.Object(str(face_path))\n",
        "                object.put(Body=buf.getvalue())\n",
        "                faces.append({\n",
        "                    'path': str(Path(video_df['path']).joinpath('frame_{}.jpg'.format(frame_idx[i]))),\n",
        "                    'label': video_df['label'],\n",
        "                    'video': video_idx,\n",
        "                    'original': video_df['original'],\n",
        "                    'frame_index': frame_idx[i],\n",
        "                    'score': float(score.numpy()),\n",
        "                    'detection': detection[:4].cpu().numpy().astype(int)\n",
        "                })\n",
        "            # Save checkpoint\n",
        "            buf = io.BytesIO()\n",
        "            pd.DataFrame(faces).to_pickle(buf)\n",
        "            buf.seek(0)\n",
        "            bucket.upload_fileobj(buf, str(faces_checkpoint_path))\n",
        "\n",
        "        return faces\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "qIy8YsWJA-MU"
      },
      "outputs": [],
      "source": [
        "def extract_faces(source_dir, videos_df, faces_dir, faces_df, checkpoint_dir, \n",
        "                  frames_per_video=15, batch_size=32, face_size=224, thread_num=7):\n",
        "    \"\"\"\n",
        "    This function extracts all frames in the dataset that contain a face.\n",
        "    :param source_dir: the parent directory that contains the datasets\n",
        "    :param videos_df: the path of the DataFrame containing all informations \n",
        "                    about the videos in the dataset.\n",
        "    :param faces_dir: the directory path to save the extracted faces from the \n",
        "                    datasets\n",
        "    :param faces_df: the path to save the DataFrame containing all informations \n",
        "                    about the extracted faces.\n",
        "    :param checkpoint_dir: the directory path to save the DataFrame[path, label,\n",
        "                    video, original, frame_index, score, detection] of the \n",
        "                    extracted faces\n",
        "    :param frames_per_video (default = 15): number of frames to extract in each\n",
        "                    video.\n",
        "    :param batch_size (default = 16): batch size of videos to treat together.\n",
        "    :param face_size (default = 224) : each frame extracted will have the size\n",
        "                    face_size x face_size\n",
        "    :thread_num (default = 4): number of threads to be used during the \n",
        "                    extraction.\n",
        "    \"\"\"\n",
        "    # On vérifie si ffpp_faces.pkl existe\n",
        "    try:\n",
        "        s3_resource.Object(bucket_name, faces_df).load()\n",
        "        file_exists = True\n",
        "    except botocore.exceptions.ClientError as e:\n",
        "        if e.response['Error']['Code'] == \"404\":\n",
        "            file_exists = False\n",
        "        else:\n",
        "            raise\n",
        "\n",
        "    # Charger le fichier de checkpoint depuis S3 s'il existe\n",
        "    if file_exists:\n",
        "        df_faces = pickle.loads(s3_resource.Bucket(bucket_name).Object(faces_df).get()['Body'].read())\n",
        "        print('We got {} faces'.format(len(df_faces)))\n",
        "        print('Faces DataFrame Loaded')\n",
        "        return df_faces\n",
        "    \n",
        "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "    \n",
        "    print('Loading video DataFrame')\n",
        "    df_videos = pickle.loads(s3_resource.Bucket(bucket_name).Object(videos_df).get()['Body'].read())\n",
        "    \n",
        "    print('Loading Blazeface model')\n",
        "    blazeface_net = BlazeFace().to(device)\n",
        "    blazeface_net.load_weights(BLAZEFACE_WEIGHTS)\n",
        "    blazeface_net.load_anchors(BLAZEFACE_ANCHORS)    \n",
        "    blazeface_net.min_score_thresh = 0.8\n",
        "    \n",
        "    ## Face extraction\n",
        "    with ThreadPoolExecutor(thread_num) as pool:\n",
        "        for batch_idx0 in tqdm(np.arange(start=0, stop=len(df_videos), step=batch_size),\n",
        "                               desc='Extracting faces'):\n",
        "            list(pool.map(partial(extract_faces_on_video,\n",
        "                          source_dir=source_dir,\n",
        "                          faces_dir=faces_dir,\n",
        "                          checkpoint_dir=checkpoint_dir,\n",
        "                          blazeface=blazeface_net,\n",
        "                          num_frames=frames_per_video,\n",
        "                          face_size=face_size,\n",
        "                          ),\n",
        "                          df_videos.iloc[batch_idx0:batch_idx0 + batch_size].iterrows()))\n",
        "    \n",
        "    faces_dataset = []\n",
        "    for _, df in tqdm(df_videos.iterrows(), total=len(df_videos), desc='Collecting faces results'):\n",
        "        face_checkpoint = Path(checkpoint_dir).joinpath(df['path'].split('.')[0] + '_faces.pkl')\n",
        "        try:\n",
        "            s3_resource.Object(bucket_name, str(face_checkpoint)).load()\n",
        "            face_checkpoint_exists = True\n",
        "        except botocore.exceptions.ClientError as e:\n",
        "            if e.response['Error']['Code'] == \"404\":\n",
        "                file_exists = False\n",
        "            else:\n",
        "                raise\n",
        "        if face_checkpoint_exists:\n",
        "            df_face = pickle.loads(s3_resource.Bucket(bucket_name).Object(str(face_checkpoint)).get()['Body'].read())\n",
        "            faces_dataset.append(df_face)\n",
        "        else:\n",
        "            print(f'Checkpoint file {face_checkpoint} does not exist')\n",
        "            \n",
        "    df_faces = pd.concat(faces_dataset, axis=0)\n",
        "    buf = io.BytesIO()\n",
        "    df_faces.to_pickle(buf)\n",
        "    buf.seek(0)\n",
        "    bucket.upload_fileobj(buf, faces_df)\n",
        "    print('We got {} faces'.format(len(df_faces)))\n",
        "    print('Completed!')\n",
        "    return df_faces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "Wy0Y5AwtA-Mb",
        "outputId": "9767bcf6-8915-4771-9682-4f7f5fad372a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "We got 44994 faces\n",
            "Faces DataFrame Loaded\n"
          ]
        }
      ],
      "source": [
        "# Run extraction\n",
        "df_faces = extract_faces(FFPP_SRC, VIDEODF_SRC, FACES_DST, FACESDF_DST,  CHECKPOINT_DST)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIzeeGplA-NQ"
      },
      "source": [
        "## Dataset processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "6_y-9WSIA-NR"
      },
      "outputs": [],
      "source": [
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 128"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hb3FfzYeA-NU"
      },
      "source": [
        "### Split dataset on original video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "PREPROCESSED_DST = 'preprocessed_data'\n",
        "\n",
        "def copy_preprocessed_data(prefix, keys):\n",
        "    for key in tqdm(keys) :\n",
        "        copy_source = {\n",
        "            'Bucket': bucket_name,\n",
        "            'Key': f'{FACES_DST}/{key}'\n",
        "        }\n",
        "        s3_resource.meta.client.copy(copy_source, bucket_name, f'{PREPROCESSED_DST}/{prefix}/{key}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "rLAoN8D1A-NU"
      },
      "outputs": [],
      "source": [
        "def make_splits(df_faces, train_ratio=0.7, val_ratio=0.15):\n",
        "    \"\"\"\n",
        "    Splitting the dataset into three subsets : train, validation and test.\n",
        "    \"\"\"\n",
        "    random_original_videos = np.random.permutation(df_faces[(df_faces['label'] == 0)]['video'].unique())\n",
        "    train_num = int(len(random_original_videos) * train_ratio)\n",
        "    val_num = int(len(random_original_videos) * val_ratio)\n",
        "    train_original = random_original_videos[:train_num]\n",
        "    val_original = random_original_videos[train_num: train_num + val_num]\n",
        "    test_original = random_original_videos[train_num + val_num:]\n",
        "    \n",
        "    df_train = pd.concat([df_faces[df_faces['original'].isin(train_original)], \n",
        "                          df_faces[df_faces['video'].isin(train_original)]], ignore_index=True)\n",
        "    df_val = pd.concat([df_faces[df_faces['original'].isin(val_original)], \n",
        "                        df_faces[df_faces['video'].isin(val_original)]], ignore_index=True)\n",
        "    df_test = pd.concat([df_faces[df_faces['original'].isin(test_original)], \n",
        "                         df_faces[df_faces['video'].isin(test_original)]], ignore_index=True)\n",
        "    \n",
        "    return df_train, df_val, df_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "dtLSibzXCXIA"
      },
      "outputs": [],
      "source": [
        "df_faces = pickle.loads(s3_resource.Bucket(bucket_name).Object(FACESDF_DST).get()['Body'].read())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "2FdyPSuBChgk",
        "outputId": "c538a570-93ac-40f5-f9c3-c2985a55d974"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>path</th>\n",
              "      <th>label</th>\n",
              "      <th>video</th>\n",
              "      <th>original</th>\n",
              "      <th>frame_index</th>\n",
              "      <th>score</th>\n",
              "      <th>detection</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>manipulated_sequences/Deepfakes/c23/videos/000...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.967828</td>\n",
              "      <td>[53, 222, 270, 408]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>manipulated_sequences/Deepfakes/c23/videos/000...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2000</td>\n",
              "      <td>28</td>\n",
              "      <td>0.977729</td>\n",
              "      <td>[51, 223, 273, 414]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>manipulated_sequences/Deepfakes/c23/videos/000...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2000</td>\n",
              "      <td>56</td>\n",
              "      <td>0.961462</td>\n",
              "      <td>[51, 232, 276, 425]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>manipulated_sequences/Deepfakes/c23/videos/000...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2000</td>\n",
              "      <td>84</td>\n",
              "      <td>0.949439</td>\n",
              "      <td>[49, 234, 268, 422]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>manipulated_sequences/Deepfakes/c23/videos/000...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2000</td>\n",
              "      <td>112</td>\n",
              "      <td>0.975719</td>\n",
              "      <td>[50, 233, 267, 419]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                path  label  video  original  \\\n",
              "0  manipulated_sequences/Deepfakes/c23/videos/000...      1      0      2000   \n",
              "1  manipulated_sequences/Deepfakes/c23/videos/000...      1      0      2000   \n",
              "2  manipulated_sequences/Deepfakes/c23/videos/000...      1      0      2000   \n",
              "3  manipulated_sequences/Deepfakes/c23/videos/000...      1      0      2000   \n",
              "4  manipulated_sequences/Deepfakes/c23/videos/000...      1      0      2000   \n",
              "\n",
              "   frame_index     score            detection  \n",
              "0            0  0.967828  [53, 222, 270, 408]  \n",
              "1           28  0.977729  [51, 223, 273, 414]  \n",
              "2           56  0.961462  [51, 232, 276, 425]  \n",
              "3           84  0.949439  [49, 234, 268, 422]  \n",
              "4          112  0.975719  [50, 233, 267, 419]  "
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_faces.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "paths = df_faces['path']\n",
        "dir_paths = [os.path.dirname(p) for p in paths]\n",
        "df_faces['dir_path'] = dir_paths\n",
        "df_faces['s3_path'] = df_faces['dir_path'].map(lambda x: f's3://{bucket_name}/{FACES_DST}/{x}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "KLy4qAuQA-NZ"
      },
      "outputs": [],
      "source": [
        "df_train, df_val, df_test = make_splits(df_faces)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "id": "AlpQAHFPA-Nc",
        "outputId": "dfdbc799-3d9e-4b5a-e550-53630693cfc6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total dataset : 44994\n",
            "Total train : 31500\n",
            "Total val : 6745\n",
            "Total test : 6749\n"
          ]
        }
      ],
      "source": [
        "r_train, _ = df_train.shape\n",
        "r_val, _ = df_val.shape\n",
        "r_test, _ = df_test.shape\n",
        "\n",
        "print('Total dataset : {}'.format(r_train+r_val+r_test))\n",
        "print('Total train : {}'.format(r_train))\n",
        "print('Total val : {}'.format(r_val))\n",
        "print('Total test : {}'.format(r_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# copy train, valid and test to specific folder \n",
        "copy_preprocessed_data('train', df_train['path'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pVM8ty6A-Nf"
      },
      "source": [
        "### Transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "RR0i7TC5A-Nf"
      },
      "outputs": [],
      "source": [
        "pre_trained_mean, pre_trained_std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomAffine(degrees=40, scale=(.9, 1.1), shear=0),\n",
        "    transforms.RandomPerspective(distortion_scale=0.2),\n",
        "    transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.RandomErasing(scale=(0.02, 0.16), ratio=(0.3, 1.6)),\n",
        "    transforms.Normalize(mean=pre_trained_mean, std=pre_trained_std)\n",
        "])\n",
        "\n",
        "val_tranform = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=pre_trained_mean, std=pre_trained_std)\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8iG1vBAA-Ni"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "faces_dir = Path(FACES_DST)\n",
        "img_path = df_faces.iloc[1]['path']\n",
        "\n",
        "object = bucket.Object(str(faces_dir.joinpath('original_sequences/youtube/c23/videos/341.mp4/frame_177.jpg')))\n",
        "response = object.get()\n",
        "file_stream = response['Body']\n",
        "Image.open(file_stream)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "_YB92wiiA-Nj"
      },
      "outputs": [],
      "source": [
        "class FFPPDataset(Dataset):\n",
        "    def __init__(self, df_faces, faces_dir=FACES_DST, transform=None):\n",
        "        super().__init__()\n",
        "        self.faces_dir = Path(faces_dir)\n",
        "        self.data, self.targets = df_faces['path'], df_faces['label']\n",
        "        self.transform = transform\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        img_path, target = self.data[index], self.targets[index]\n",
        "        target = np.array([target,]).astype(np.float32)\n",
        "        \n",
        "        file_stream = bucket.Object(str(self.faces_dir.joinpath(img_path))).get()['Body']\n",
        "        img = Image.open(file_stream)\n",
        "        \n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "        return img, target\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "paths = IterableWrapper(df_faces['s3_path'].unique().tolist()).list_files_by_s3()\n",
        "sharded_s3_urls = paths.shuffle().sharding_filter()\n",
        "dp_s3_files = S3FileLoader(sharded_s3_urls)\n",
        "for url, stream in dp_s3_files:\n",
        "    print(url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import IterableDataset\n",
        "from torchdata.datapipes.iter import IterableWrapper, S3FileLoader\n",
        "\n",
        "class S3FFPPDataset(IterableDataset):\n",
        "    def __init__(self, df, faces_dir=FACES_DST, shuffle_urls=False, transform=None):\n",
        "        super().__init__()\n",
        "        self.data = df\n",
        "        self.paths = IterableWrapper(df['s3_path'].unique().tolist()).list_files_by_s3()\n",
        "        self.targets = IterableWrapper(df['label'])\n",
        "        if shuffle_urls:\n",
        "            self.sharded_s3_urls = self.paths.shuffle().sharding_filter()\n",
        "            self.s3_files = S3FileLoader(self.sharded_s3_urls)\n",
        "        else:\n",
        "            self.s3_files = S3FileLoader(self.paths)\n",
        "        self.transform = transform\n",
        "    \n",
        "    def data_generator(self):\n",
        "        try:\n",
        "            while True:\n",
        "                url, stream = next(self.s3_files_iterator)\n",
        "                target = next(self.targets_iterator)\n",
        "                \n",
        "                target = np.array([target,]).astype(np.float32)\n",
        "                img = Image.open(stream)\n",
        "                \n",
        "                if self.transform is not None:\n",
        "                    img = self.transform(img)\n",
        "                yield img, target\n",
        "\n",
        "        except StopIteration:\n",
        "            return\n",
        "\n",
        "    def __iter__(self):\n",
        "        self.s3_files_iterator = iter(self.s3_files)\n",
        "        self.targets_iterator = iter(self.targets)\n",
        "        return self.data_generator()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "rxBdI_boA-Nm"
      },
      "outputs": [],
      "source": [
        "train_dataset = S3FFPPDataset(df_train, shuffle_urls=True, transform=train_transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, \n",
        "                          num_workers=0)\n",
        "\n",
        "valid_dataset = S3FFPPDataset(df_val, transform=val_tranform)\n",
        "val_loader = DataLoader(valid_dataset, batch_size=32, num_workers=0)\n",
        "\n",
        "test_dataset = S3FFPPDataset(df_test, transform=val_tranform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, num_workers=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[tensor([[[[-1.5528, -1.5528, -1.5528,  ..., -1.5528, -1.5528, -1.5528],\n",
              "           [-1.5528, -1.5528, -1.5528,  ..., -1.5528, -1.5528, -1.5528],\n",
              "           [-1.5528, -1.5528, -1.5528,  ..., -1.5528, -1.5528, -1.5528],\n",
              "           ...,\n",
              "           [-1.5528, -1.5528, -1.5528,  ..., -1.5528, -1.5528, -1.5528],\n",
              "           [-1.5528, -1.5528, -1.5528,  ..., -1.5528, -1.5528, -1.5528],\n",
              "           [-1.5528, -1.5528, -1.5528,  ..., -1.5528, -1.5528, -1.5528]],\n",
              " \n",
              "          [[-1.4580, -1.4580, -1.4580,  ..., -1.4580, -1.4580, -1.4580],\n",
              "           [-1.4580, -1.4580, -1.4580,  ..., -1.4580, -1.4580, -1.4580],\n",
              "           [-1.4580, -1.4580, -1.4580,  ..., -1.4580, -1.4580, -1.4580],\n",
              "           ...,\n",
              "           [-1.4580, -1.4580, -1.4580,  ..., -1.4580, -1.4580, -1.4580],\n",
              "           [-1.4580, -1.4580, -1.4580,  ..., -1.4580, -1.4580, -1.4580],\n",
              "           [-1.4580, -1.4580, -1.4580,  ..., -1.4580, -1.4580, -1.4580]],\n",
              " \n",
              "          [[-1.2293, -1.2293, -1.2293,  ..., -1.2293, -1.2293, -1.2293],\n",
              "           [-1.2293, -1.2293, -1.2293,  ..., -1.2293, -1.2293, -1.2293],\n",
              "           [-1.2293, -1.2293, -1.2293,  ..., -1.2293, -1.2293, -1.2293],\n",
              "           ...,\n",
              "           [-1.2293, -1.2293, -1.2293,  ..., -1.2293, -1.2293, -1.2293],\n",
              "           [-1.2293, -1.2293, -1.2293,  ..., -1.2293, -1.2293, -1.2293],\n",
              "           [-1.2293, -1.2293, -1.2293,  ..., -1.2293, -1.2293, -1.2293]]],\n",
              " \n",
              " \n",
              "         [[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
              "           [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
              "           [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
              "           ...,\n",
              "           [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
              "           [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
              "           [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
              " \n",
              "          [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
              "           [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
              "           [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
              "           ...,\n",
              "           [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
              "           [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
              "           [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
              " \n",
              "          [[-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
              "           [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
              "           [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
              "           ...,\n",
              "           [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
              "           [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
              "           [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]],\n",
              " \n",
              " \n",
              "         [[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
              "           [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
              "           [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
              "           ...,\n",
              "           [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
              "           [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
              "           [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
              " \n",
              "          [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
              "           [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
              "           [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
              "           ...,\n",
              "           [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
              "           [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
              "           [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
              " \n",
              "          [[-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
              "           [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
              "           [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
              "           ...,\n",
              "           [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
              "           [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
              "           [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]],\n",
              " \n",
              " \n",
              "         ...,\n",
              " \n",
              " \n",
              "         [[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
              "           [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
              "           [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
              "           ...,\n",
              "           [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
              "           [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
              "           [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
              " \n",
              "          [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
              "           [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
              "           [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
              "           ...,\n",
              "           [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
              "           [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
              "           [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
              " \n",
              "          [[-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
              "           [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
              "           [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
              "           ...,\n",
              "           [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
              "           [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
              "           [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]],\n",
              " \n",
              " \n",
              "         [[[-1.5357, -1.5357, -1.5357,  ..., -1.6727, -1.6042, -1.5528],\n",
              "           [-1.6213, -1.6042, -1.6213,  ..., -1.5699, -1.5185, -1.4672],\n",
              "           [-1.6555, -1.6384, -1.6384,  ..., -1.5699, -1.4329, -1.3644],\n",
              "           ...,\n",
              "           [-1.3815, -1.4158, -1.4158,  ..., -1.5528, -1.5528, -1.5699],\n",
              "           [-1.3302, -1.3644, -1.4158,  ..., -1.5528, -1.5528, -1.5528],\n",
              "           [-1.2788, -1.3130, -1.3473,  ..., -1.5014, -1.5185, -1.5528]],\n",
              " \n",
              "          [[-1.3880, -1.3880, -1.3880,  ..., -1.5280, -1.4755, -1.4230],\n",
              "           [-1.4755, -1.4580, -1.4755,  ..., -1.4230, -1.3704, -1.3179],\n",
              "           [-1.4930, -1.4930, -1.4930,  ..., -1.4230, -1.2654, -1.1954],\n",
              "           ...,\n",
              "           [-1.3880, -1.4405, -1.4580,  ..., -1.5980, -1.5980, -1.6155],\n",
              "           [-1.3529, -1.3880, -1.4580,  ..., -1.5805, -1.5805, -1.5980],\n",
              "           [-1.3004, -1.3354, -1.3880,  ..., -1.5280, -1.5455, -1.5805]],\n",
              " \n",
              "          [[-0.7761, -0.7761, -0.7761,  ..., -1.1944, -1.1421, -1.0898],\n",
              "           [-0.8633, -0.8458, -0.8633,  ..., -1.1073, -1.0550, -1.0027],\n",
              "           [-0.8807, -0.8807, -0.8807,  ..., -1.1073, -0.9504, -0.8807],\n",
              "           ...,\n",
              "           [-1.2990, -1.3339, -1.3339,  ..., -1.4036, -1.4036, -1.4210],\n",
              "           [-1.2467, -1.2990, -1.3339,  ..., -1.3861, -1.4036, -1.4210],\n",
              "           [-1.1944, -1.2293, -1.2641,  ..., -1.3339, -1.3513, -1.4036]]],\n",
              " \n",
              " \n",
              "         [[[-1.9467, -1.9467, -1.9467,  ..., -1.9467, -1.9467, -1.9467],\n",
              "           [-1.9467, -1.9467, -1.9467,  ..., -1.9467, -1.9467, -1.9467],\n",
              "           [-1.9467, -1.9467, -1.9467,  ..., -1.9467, -1.9467, -1.9467],\n",
              "           ...,\n",
              "           [-1.9467, -1.9467, -1.9467,  ..., -1.9467, -1.9467, -1.9467],\n",
              "           [-1.9467, -1.9467, -1.9467,  ..., -1.9467, -1.9467, -1.9467],\n",
              "           [-1.9467, -1.9467, -1.9467,  ..., -1.9467, -1.9467, -1.9467]],\n",
              " \n",
              "          [[-1.8606, -1.8606, -1.8606,  ..., -1.8606, -1.8606, -1.8606],\n",
              "           [-1.8606, -1.8606, -1.8606,  ..., -1.8606, -1.8606, -1.8606],\n",
              "           [-1.8606, -1.8606, -1.8606,  ..., -1.8606, -1.8606, -1.8606],\n",
              "           ...,\n",
              "           [-1.8606, -1.8606, -1.8606,  ..., -1.8606, -1.8606, -1.8606],\n",
              "           [-1.8606, -1.8606, -1.8606,  ..., -1.8606, -1.8606, -1.8606],\n",
              "           [-1.8606, -1.8606, -1.8606,  ..., -1.8606, -1.8606, -1.8606]],\n",
              " \n",
              "          [[-1.6302, -1.6302, -1.6302,  ..., -1.6302, -1.6302, -1.6302],\n",
              "           [-1.6302, -1.6302, -1.6302,  ..., -1.6302, -1.6302, -1.6302],\n",
              "           [-1.6302, -1.6302, -1.6302,  ..., -1.6302, -1.6302, -1.6302],\n",
              "           ...,\n",
              "           [-1.6302, -1.6302, -1.6302,  ..., -1.6302, -1.6302, -1.6302],\n",
              "           [-1.6302, -1.6302, -1.6302,  ..., -1.6302, -1.6302, -1.6302],\n",
              "           [-1.6302, -1.6302, -1.6302,  ..., -1.6302, -1.6302, -1.6302]]]]),\n",
              " tensor([[1.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [1.]])]"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "batch_train = next(iter(train_loader))\n",
        "batch_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7swTRXvMbld"
      },
      "source": [
        "## Entraîner le réseau EfficientNet\n",
        "\n",
        "Modèle pré-entraîner nommé : efficientnet-b4\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "BVLsLQ3N1Ha5"
      },
      "outputs": [],
      "source": [
        "# Training configuration\n",
        "# number of epochs to train for\n",
        "num_epochs = 10\n",
        "\n",
        "# learning rate\n",
        "learning_rate = 1e-4\n",
        "\n",
        "# loss_function\n",
        "criterion = nn.BCEWithLogitsLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "8NTgHso71zkG"
      },
      "outputs": [],
      "source": [
        "def train(net, model_name):\n",
        "  base_path = 'model_checkpoint'\n",
        "  model_path = os.path.join(base_path, \n",
        "                            f'models/{model_name}' + '/best_epoch_{epoch}.ckpt')\n",
        "  Path(model_path).parent.mkdir(parents=True, exist_ok=True)\n",
        "  logs_file = os.path.join(base_path, f'logs/{model_name}/log.tsv')\n",
        "  Path(logs_file).parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "  callbacks = [\n",
        "    # Save the weights in a new file when the current model is better than all \n",
        "    # previous models.\n",
        "    ModelCheckpoint(model_path, monitor='val_bin_acc', mode='max', \n",
        "                    save_best_only=True, restore_best=True, verbose=True, \n",
        "                    temporary_filename=model_name + '_best_epoch.ckpt.tmp'),\n",
        "    \n",
        "    # Save the losses and accuracies for each epoch in a TSV.\n",
        "    CSVLogger(logs_file, separator='\\t'),\n",
        "  ]\n",
        "\n",
        "  params = (p for p in net.parameters() if p.requires_grad)\n",
        "  optimizer = optim.Adam(params, lr=learning_rate, weight_decay=0.000001)\n",
        "\n",
        "  model = Model(net, optimizer, criterion, batch_metrics=['bin_acc'])\n",
        "  model.to(device)\n",
        "\n",
        "  if Path(logs_file).exists():\n",
        "    logs = pd.read_csv(logs_file, sep='\\t')\n",
        "    epochs = list(logs['epoch'])\n",
        "    if len(epochs) != 0:\n",
        "      best_epoch_idx = logs['val_bin_acc'].idxmax()\n",
        "      best_epoch = int(logs.loc[best_epoch_idx]['epoch'])\n",
        "      model.load_weights(model_path.format(epoch=best_epoch))\n",
        "      if num_epochs not in epochs:\n",
        "        # Train\n",
        "        model.fit_generator(train_loader, val_loader, epochs=num_epochs, initial_epoch=epochs[-1], callbacks=callbacks)\n",
        "    else:\n",
        "      model.fit_generator(train_loader, val_loader, epochs=num_epochs, callbacks=callbacks)\n",
        "  else:\n",
        "    model.fit_generator(train_loader, val_loader, epochs=num_epochs, callbacks=callbacks)\n",
        "\n",
        "  logs = pd.read_csv(logs_file, sep='\\t')\n",
        "\n",
        "  best_epoch_idx = logs['val_bin_acc'].idxmax()\n",
        "  best_epoch = int(logs.loc[best_epoch_idx]['epoch'])\n",
        "  print(\"Best epoch: %d\" % best_epoch)\n",
        "\n",
        "  metrics = ['loss', 'val_loss']\n",
        "  plt.plot(logs['epoch'], logs[metrics])\n",
        "  plt.legend(metrics)\n",
        "  plt.title('Loss')\n",
        "  plt.show()\n",
        "\n",
        "  metrics = ['bin_acc', 'val_bin_acc']\n",
        "  plt.plot(logs['epoch'], logs[metrics])\n",
        "  plt.legend(metrics)\n",
        "  plt.title('Accuracy')\n",
        "  plt.show()\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "nKbzkBCb-vQ3"
      },
      "outputs": [],
      "source": [
        "def test(model, test_loader):\n",
        "  test_loss, test_acc = model.evaluate_generator(test_loader)\n",
        "  print('Test:\\n\\tLoss: {}\\n\\tAccuracy: {}'.format(test_loss, test_acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oc8EM1TS1FyQ"
      },
      "source": [
        "### Freezant tous les paramètres de convolution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 630
        },
        "id": "gkzbm95FFODY",
        "outputId": "5cdc683b-a6c8-45e5-c17b-6ca6ac962b2e"
      },
      "outputs": [],
      "source": [
        "efficientnet_a = EfficientNet.from_pretrained('efficientnet-b4')\n",
        "efficientnet_a._fc = nn.Linear(efficientnet_a._conv_head.out_channels, 1)\n",
        "\n",
        "for name, param in efficientnet_a.named_parameters():\n",
        "  if not name.startswith('_fc'):\n",
        "    param.requires_grad = False\n",
        "\n",
        "model_a = train(efficientnet_a, 'efficientnet_a')\n",
        "test(model_a, test_loader)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "T5_FQ-nrQAYz"
      },
      "source": [
        "### En réentraînant quelques blocs\n",
        "\n",
        "Réentraîner les 8 derniers blocs avec les deux dernières couches (couche de convolution et classificateur)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "070eb8acf3da4a90b422357a557c02de",
            "a97360270ec9405185501d1d5d1dcb49",
            "be1a54c2a1584186aad0ccb38ce488e6",
            "d1e20ce9d6334d43bdca7187103ba40b",
            "1b7d182d4e714faea9e3f810e5432408",
            "14f55ce43863405ab8adbe64cbbc86fb",
            "e58c63dc8ef649afb481b3f92aafd1fd",
            "d3c6935c60c84e2a857ac0040d837288"
          ]
        },
        "id": "mGvhRsFfULPj",
        "outputId": "49589859-6fe6-47f7-ee04-9b75ffb02010"
      },
      "outputs": [],
      "source": [
        "efficientnet_b = EfficientNet.from_pretrained('efficientnet-b4')\n",
        "efficientnet_b._fc = nn.Linear(efficientnet_b._conv_head.out_channels, 1)\n",
        "\n",
        "names = [name for name, _ in list(efficientnet_b.named_parameters())[-109:]]\n",
        "for name, param in efficientnet_b.named_parameters():\n",
        "  if name not in names:\n",
        "    param.requires_grad = False\n",
        "\n",
        "model_b = train(efficientnet_b, 'efficientnet_b')\n",
        "test(model_b, test_loader)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Tq3kPtqzmEsJ"
      },
      "source": [
        "Réentraîner les 16 derniers blocs avec les deux dernières couches (couche de convolution et classificateur)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "69494fc6ec25404aafdb3039e766ced3",
            "fd12c8734c9e4366bb43665fa565d57e",
            "556ecf641eb744929281c8b94c929da6",
            "2477092084fe428fb571e8f9968ad796",
            "d5d44796e5014f48a389ab9717f1d9fa",
            "5438b806f5c248a6ada5935522b7d4bb",
            "4f780573f05d4d1a9668ca7f80921bf2",
            "951256577b0c45dea9278149903d5343"
          ]
        },
        "id": "e2phbJ577fhQ",
        "outputId": "cece2b2a-04c6-4c60-d4d4-38266a0a6f02"
      },
      "outputs": [],
      "source": [
        "efficientnet_c = EfficientNet.from_pretrained('efficientnet-b4')\n",
        "efficientnet_c._fc = nn.Linear(efficientnet_c._conv_head.out_channels, 1)\n",
        "\n",
        "names = [name for name, _ in list(efficientnet_c.named_parameters())[-213:]]\n",
        "for name, param in efficientnet_c.named_parameters():\n",
        "  if name not in names:\n",
        "    param.requires_grad = False\n",
        "\n",
        "model_c = train(efficientnet_c, 'efficientnet_c')\n",
        "test(model_c, test_loader)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_tQJWTsnhWtk"
      },
      "source": [
        "## Mécanisme d'attention\n",
        "\n",
        "Ajouter un mécanisme d'attention visuelle : Spatial Transformer Network (Applique une transformation spatiale sur les\n",
        "feature maps, lors du forward pass) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WhHp-pz8qX1X"
      },
      "outputs": [],
      "source": [
        "class EfficientNetAtt(EfficientNet):\n",
        "    def __init__(self, blocks_args=None, global_params=None):\n",
        "        super().__init__(blocks_args, global_params)\n",
        "        \n",
        "    def init_spatial_transformer(self):\n",
        "        self.block_idx = 20\n",
        "        \n",
        "        # Spatial transformer localization-network\n",
        "        self.localization = nn.Sequential(\n",
        "            nn.Conv2d(160, 160, kernel_size=5),\n",
        "            nn.MaxPool2d(2, stride=2),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(160, 80, kernel_size=3),\n",
        "            nn.MaxPool2d(2, stride=2),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "        \n",
        "        # Regressor for the 3 * 2 affine matrix\n",
        "        self.fc_loc = nn.Sequential(\n",
        "            nn.Linear(80 * 1 * 1, 32),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(32, 3 * 2)\n",
        "        )\n",
        "        \n",
        "        # Initialize the weights/bias with identity transformation\n",
        "        self.fc_loc[2].weight.data.zero_()\n",
        "        self.fc_loc[2].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], \n",
        "                                                    dtype=torch.float))\n",
        "    \n",
        "    def stn(self, inputs):\n",
        "        inputs = inputs.clone()\n",
        "        xs = self.localization(inputs)\n",
        "        xs = xs.view(-1, 80 * 1 * 1)\n",
        "        theta = self.fc_loc(xs)\n",
        "        theta = theta.view(-1, 2, 3)\n",
        "\n",
        "        grid = F.affine_grid(theta, inputs.size(), align_corners=False)\n",
        "        x = F.grid_sample(inputs, grid, align_corners=False)\n",
        "\n",
        "        return x\n",
        "    \n",
        "    def extract_features(self, inputs):\n",
        "        \"\"\" Returns output of the final convolution layer \"\"\"\n",
        "\n",
        "        # Stem\n",
        "        x = self._swish(self._bn0(self._conv_stem(inputs)))\n",
        "\n",
        "        # Blocks\n",
        "        for idx, block in enumerate(self._blocks):\n",
        "            drop_connect_rate = self._global_params.drop_connect_rate\n",
        "            if drop_connect_rate:\n",
        "                drop_connect_rate *= float(idx) / len(self._blocks)\n",
        "            x = block(x, drop_connect_rate=drop_connect_rate)\n",
        "            if idx == self.block_idx:\n",
        "                x *= self.stn(x)\n",
        "\n",
        "        # Head\n",
        "        x = self._swish(self._bn1(self._conv_head(x)))\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "b7ac4ff7d8004461b9ea3b5d83cee0b1",
            "f54f6addc4494529a4f8b7274aa98117",
            "52e77c570ca1490d9912a0ddc085e810",
            "35ed6513b0024b709df00f26e9b3cea4",
            "e4aeeb499d5a443baa963a3ed489d7ae",
            "0085da15665a4a46a08eb67d95457101",
            "182db4cbdac549f8840138a5dab43d58",
            "5e4d16b0351b4c1d85b9b883c2a65a8a"
          ]
        },
        "id": "uCefQSQCiKKa",
        "outputId": "69c2662a-bdb4-48e0-e448-8f87f91ff34c"
      },
      "outputs": [],
      "source": [
        "efficientnet_d = EfficientNetAtt.from_pretrained('efficientnet-b4')\n",
        "efficientnet_d._fc = nn.Linear(efficientnet_d._conv_head.out_channels, 1)\n",
        "efficientnet_d.init_spatial_transformer()\n",
        "\n",
        "names = [name for name, _ in list(efficientnet_d.named_parameters())[-221:]]\n",
        "for name, param in efficientnet_d.named_parameters():\n",
        "  if name not in names:\n",
        "    param.requires_grad = False\n",
        "\n",
        "model_d = train(efficientnet_d, 'efficientnet_d')\n",
        "test(model_d, test_loader)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "torch-gpu",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8 (default, Apr 13 2021, 12:59:45) \n[Clang 10.0.0 ]"
    },
    "vscode": {
      "interpreter": {
        "hash": "3bc196b68f548d1b4a7b23b62dc36c96d4883bf50034a10858aedfd454af5fac"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0085da15665a4a46a08eb67d95457101": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "070eb8acf3da4a90b422357a557c02de": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_be1a54c2a1584186aad0ccb38ce488e6",
              "IPY_MODEL_d1e20ce9d6334d43bdca7187103ba40b"
            ],
            "layout": "IPY_MODEL_a97360270ec9405185501d1d5d1dcb49"
          }
        },
        "14f55ce43863405ab8adbe64cbbc86fb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "182db4cbdac549f8840138a5dab43d58": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1b7d182d4e714faea9e3f810e5432408": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "2477092084fe428fb571e8f9968ad796": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_951256577b0c45dea9278149903d5343",
            "placeholder": "​",
            "style": "IPY_MODEL_4f780573f05d4d1a9668ca7f80921bf2",
            "value": " 74.4M/74.4M [00:05&lt;00:00, 13.7MB/s]"
          }
        },
        "35ed6513b0024b709df00f26e9b3cea4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e4d16b0351b4c1d85b9b883c2a65a8a",
            "placeholder": "​",
            "style": "IPY_MODEL_182db4cbdac549f8840138a5dab43d58",
            "value": " 74.4M/74.4M [00:05&lt;00:00, 13.2MB/s]"
          }
        },
        "4f780573f05d4d1a9668ca7f80921bf2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "52e77c570ca1490d9912a0ddc085e810": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0085da15665a4a46a08eb67d95457101",
            "max": 77999237,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e4aeeb499d5a443baa963a3ed489d7ae",
            "value": 77999237
          }
        },
        "5438b806f5c248a6ada5935522b7d4bb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "556ecf641eb744929281c8b94c929da6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5438b806f5c248a6ada5935522b7d4bb",
            "max": 77999237,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d5d44796e5014f48a389ab9717f1d9fa",
            "value": 77999237
          }
        },
        "5e4d16b0351b4c1d85b9b883c2a65a8a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69494fc6ec25404aafdb3039e766ced3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_556ecf641eb744929281c8b94c929da6",
              "IPY_MODEL_2477092084fe428fb571e8f9968ad796"
            ],
            "layout": "IPY_MODEL_fd12c8734c9e4366bb43665fa565d57e"
          }
        },
        "951256577b0c45dea9278149903d5343": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a97360270ec9405185501d1d5d1dcb49": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7ac4ff7d8004461b9ea3b5d83cee0b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_52e77c570ca1490d9912a0ddc085e810",
              "IPY_MODEL_35ed6513b0024b709df00f26e9b3cea4"
            ],
            "layout": "IPY_MODEL_f54f6addc4494529a4f8b7274aa98117"
          }
        },
        "be1a54c2a1584186aad0ccb38ce488e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_14f55ce43863405ab8adbe64cbbc86fb",
            "max": 77999237,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1b7d182d4e714faea9e3f810e5432408",
            "value": 77999237
          }
        },
        "d1e20ce9d6334d43bdca7187103ba40b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d3c6935c60c84e2a857ac0040d837288",
            "placeholder": "​",
            "style": "IPY_MODEL_e58c63dc8ef649afb481b3f92aafd1fd",
            "value": " 74.4M/74.4M [00:00&lt;00:00, 102MB/s]"
          }
        },
        "d3c6935c60c84e2a857ac0040d837288": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5d44796e5014f48a389ab9717f1d9fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "e4aeeb499d5a443baa963a3ed489d7ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "e58c63dc8ef649afb481b3f92aafd1fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f54f6addc4494529a4f8b7274aa98117": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd12c8734c9e4366bb43665fa565d57e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
