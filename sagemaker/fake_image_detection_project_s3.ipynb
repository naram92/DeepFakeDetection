{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b4\n",
      "Iteration: 100%|██████████████████████| 7/7 [00:55<00:00,  9.25s/it, loss=0.723]\n",
      "Training Results - Epoch: 1  Avg accuracy: 0.49 Avg loss: 0.69\n",
      "Validation Results - Epoch: 1  Avg accuracy: 0.27 Avg loss: 0.71\n",
      "save model to torchscript\n"
     ]
    }
   ],
   "source": [
    "!python ./code/test_train.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading neumf model.\n",
      "Loading neumf model.\n",
      "Engine run starting with max_epochs=1.\n",
      "Epoch[1] Complete. Time taken: 00:00:32.482\n",
      "Validation {'accuracy': 0.6111111111111112, 'loss': 0.6834699842664931}\n",
      "Validation {'accuracy': 0.6111111111111112, 'loss': 0.6834699842664931}\n",
      "Engine run complete. Time taken: 00:00:32.513\n",
      "Accuracy: 0.6111111111111112\n",
      "Accuracy: 0.6111111111111112\n"
     ]
    }
   ],
   "source": [
    "!python ./evaluate/test_evaluate.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Yly4fQhA-LO",
    "tags": []
   },
   "source": [
    "# DEEPFAKE DETECTION PROJECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip3 install --user \"git+https://github.com/pytorch/data.git\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('/root/DeepFakeDetection/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "tWguqkOXA-Lz",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from functools import partial\n",
    "\n",
    "from blazeface import BlazeFace\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from poutyne.framework import Model, ModelCheckpoint, BestModelRestore, CSVLogger\n",
    "import boto3\n",
    "import io\n",
    "import botocore\n",
    "import tempfile\n",
    "from pathlib import PurePosixPath\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_resource = boto3.resource('s3')\n",
    "s3_client = boto3.client('s3')\n",
    "bucket_name = 'deepfake-detection'\n",
    "bucket = s3_resource.Bucket(bucket_name)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U6rFYG20A-L9"
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:00<00:00, 74.52it/s]\n",
      "100%|██████████| 11/11 [00:00<00:00, 77.97it/s]\n"
     ]
    }
   ],
   "source": [
    "manipulated_sequences = ['Deepfakes/c23/videos/220_219.mp4', 'FaceSwap/c23/videos/486_680.mp4', 'FaceSwap/c23/videos/383_353.mp4', 'NeuralTextures/c23/videos/363_343.mp4', 'NeuralTextures/c23/videos/353_383.mp4', 'Face2Face/c23/videos/343_363.mp4', 'Face2Face/c23/videos/287_426.mp4', 'Deepfakes/c23/videos/038_125.mp4', 'Face2Face/c23/videos/132_007.mp4', 'FaceSwap/c23/videos/117_217.mp4', 'NeuralTextures/c23/videos/200_189.mp4']\n",
    "manipulated_sequences_dir = Path('datasets/manipulated_sequences/')\n",
    "manipulated_sequences_dest_dir = Path('dev_datasets/manipulated_sequences/')\n",
    "for item in tqdm(manipulated_sequences):\n",
    "    file_path = manipulated_sequences_dir.joinpath(item)\n",
    "    file_dest_path = manipulated_sequences_dest_dir.joinpath(item)\n",
    "    copy_source = {\n",
    "            'Bucket': bucket_name,\n",
    "            'Key': str(file_path)\n",
    "    }\n",
    "    try:\n",
    "        s3_client.head_object(Bucket=bucket_name, Key=str(file_dest_path))\n",
    "    except botocore.exceptions.ClientError as e:\n",
    "        # L'objet n'existe pas\n",
    "        if e.response['Error']['Code'] == '404':\n",
    "            s3_resource.meta.client.copy(copy_source, bucket_name, str(file_dest_path))\n",
    "        else:\n",
    "            raise\n",
    "    else:\n",
    "        # L'objet existe déjà, passez à l'objet suivant dans la boucle\n",
    "        continue\n",
    "\n",
    "original_sequences = ['youtube/c23/videos/343.mp4', 'youtube/c23/videos/383.mp4', 'youtube/c23/videos/220.mp4', 'youtube/c23/videos/353.mp4', 'youtube/c23/videos/486.mp4', 'youtube/c23/videos/287.mp4', 'youtube/c23/videos/363.mp4', 'youtube/c23/videos/038.mp4', 'youtube/c23/videos/132.mp4', 'youtube/c23/videos/117.mp4', 'youtube/c23/videos/200.mp4']\n",
    "original_sequences_dir = Path('datasets/original_sequences/')\n",
    "original_sequences_dest_dir = Path('dev_datasets/original_sequences/')\n",
    "for item in tqdm(original_sequences):\n",
    "    file_path = original_sequences_dir.joinpath(item)\n",
    "    file_dest_path = original_sequences_dest_dir.joinpath(item)\n",
    "    copy_source = {\n",
    "            'Bucket': bucket_name,\n",
    "            'Key': str(file_path)\n",
    "    }\n",
    "    try:\n",
    "        s3_client.head_object(Bucket=bucket_name, Key=str(file_dest_path))\n",
    "    except botocore.exceptions.ClientError as e:\n",
    "        # L'objet n'existe pas\n",
    "        if e.response['Error']['Code'] == '404':\n",
    "            s3_resource.meta.client.copy(copy_source, bucket_name, str(file_dest_path))\n",
    "        else:\n",
    "            raise\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "8zrO7GjUA-L_",
    "tags": []
   },
   "outputs": [],
   "source": [
    "FFPP_SRC = 'dev_datasets/'\n",
    "VIDEODF_SRC = os.path.join(FFPP_SRC, 'ffpp_videos.pkl')\n",
    "\n",
    "BLAZEFACE_WEIGHTS = os.path.join(FFPP_SRC, 'blazeface/blazeface.pth')\n",
    "BLAZEFACE_ANCHORS = os.path.join(FFPP_SRC, 'blazeface/anchors.npy') \n",
    "\n",
    "FACES_DST = os.path.join(FFPP_SRC, 'extract_faces')\n",
    "FACESDF_DST = os.path.join(FACES_DST, 'ffpp_faces.pkl')\n",
    "CHECKPOINT_DST = os.path.join(FACES_DST, 'checkpoint')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hkMswaAHA-MF"
   },
   "source": [
    "### FF++ Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_ffpp(source_dir, video_dataset_path):\n",
    "    \"\"\"\n",
    "        Preprocessing video dataset : Set the label of each video {0 for real video, \n",
    "        1 for fake video} and the video original of fake videos.\n",
    "        :param source_dir: The parent directory that contains all videos (real or fake)\n",
    "        :param video_dataset_path: The path to save the videos DataFrame[path, name, \n",
    "                                    label, original]\n",
    "    \"\"\" \n",
    "    try:\n",
    "        s3_resource.Object(bucket_name, video_dataset_path).load()\n",
    "        file_exists = True\n",
    "    except botocore.exceptions.ClientError as e:\n",
    "        if e.response['Error']['Code'] == \"404\":\n",
    "            file_exists = False\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "    # Charger le fichier de checkpoint depuis S3 s'il existe\n",
    "    if file_exists:\n",
    "        df_videos = pickle.loads(s3_resource.Bucket(bucket_name).Object(video_dataset_path).get()['Body'].read())\n",
    "    else :\n",
    "        # Si le fichier n'existe pas dans S3, vous pouvez créer le DataFrame de la façon suivante :\n",
    "        print('Creating video DataFrame')\n",
    "        # df_videos = pd.DataFrame(columns=['path', 'label', 'name', 'original'])\n",
    "\n",
    "        # Initialisez une liste vide pour stocker les chemins des fichiers .mp4\n",
    "        mp4_files = []\n",
    "\n",
    "        # Appelez la méthode list_objects_v2 de l'objet client S3 en spécifiant le paramètre ContinuationToken\n",
    "        # lors de chaque itération, jusqu'à ce qu'il n'y ait plus d'objets à récupérer \n",
    "        objects_list = s3_client.list_objects_v2(\n",
    "            Bucket=bucket_name, Prefix=source_dir\n",
    "        )\n",
    "        while True:\n",
    "            # Parcourez la liste des objets et ajoutez le chemin de chaque fichier mp4 à la liste\n",
    "            for obj in objects_list['Contents']:\n",
    "                # Vérifiez si l'objet est un fichier mp4\n",
    "                if obj['Key'].endswith('.mp4'):\n",
    "                    # Ajoutez le chemin du fichier à la liste\n",
    "                    mp4_files.append(obj['Key'])\n",
    "            \n",
    "            # Vérifiez si il y a une suite de résultats\n",
    "            if 'NextContinuationToken' in objects_list:\n",
    "                # Si oui, récupérez la suite des résultats en spécifiant le ContinuationToken\n",
    "                objects_list = s3_client.list_objects_v2(\n",
    "                    Bucket=bucket_name, Prefix=source_dir, ContinuationToken=objects_list['NextContinuationToken']\n",
    "                )\n",
    "            else:\n",
    "                # Si non, sortez de la boucle\n",
    "                break\n",
    "        \n",
    "        # Créez le DataFrame en utilisant la liste des chemins de fichiers .mp4\n",
    "        df_videos = pd.DataFrame({'path': mp4_files})\n",
    "        # Enlevez le repertoire racine datasets/ dans le path\n",
    "        df_videos['path'] = df_videos['path'].replace('dev_datasets/', '', regex=True)\n",
    "        # Convertissez les chaînes de caractères en objets PurePosixPath\n",
    "        df_videos['path'] = df_videos['path'].apply(lambda x: PurePosixPath(x))           \n",
    "\n",
    "        # 1 if fake, otherwise 0\n",
    "        df_videos['label'] = df_videos['path'].map(\n",
    "            lambda x: 1 if x.parts[0] == 'manipulated_sequences' else 0)\n",
    "        \n",
    "        source = df_videos['path'].map(lambda x: x.parts[1]).astype('category')\n",
    "        df_videos['name'] = df_videos['path'].map(lambda x: x.with_suffix('').parts[-1])\n",
    "        df_videos['path'] = df_videos['path'].map(lambda x: str(x))\n",
    "\n",
    "        df_videos['original'] = -1 * np.ones(len(df_videos), dtype=np.int16)\n",
    "        # Mettre dans la colonne original l'index de l'original des fakes\n",
    "        df_videos.loc[(df_videos['label'] == 1) & (source != 'DeepFakeDetection'), 'original'] = \\\n",
    "            df_videos[(df_videos['label'] == 1) & (source != 'DeepFakeDetection')]['name'].map(\n",
    "                lambda x: df_videos.index[np.flatnonzero(df_videos['name'] == x.split('_')[0])[0]]\n",
    "            )\n",
    "        df_videos.loc[(df_videos['label'] == 1) & (source == 'DeepFakeDetection'), 'original'] = \\\n",
    "            df_videos[(df_videos['label'] == 1) & (source == 'DeepFakeDetection')]['name'].map(\n",
    "                lambda x: df_videos.index[\n",
    "                    np.flatnonzero(df_videos['name'] == x.split('_')[0] + '__' + x.split('__')[1])[0]]\n",
    "            )\n",
    "    \n",
    "        # Enregistrez le DataFrame dans S3\n",
    "        print('Saving video DataFrame to {}'.format(video_dataset_path))\n",
    "        buf = io.BytesIO()\n",
    "        df_videos.to_pickle(buf)\n",
    "        buf.seek(0)\n",
    "        bucket.upload_fileobj(buf, video_dataset_path)\n",
    "    \n",
    "    print('Real videos: {:d}'.format(sum(df_videos['label'] == 0)))\n",
    "    print('Fake videos: {:d}'.format(sum(df_videos['label'] == 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "id": "50I8DTgAA-MX",
    "outputId": "50299075-5427-4974-ffbd-15c48da73d86",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real videos: 11\n",
      "Fake videos: 11\n"
     ]
    }
   ],
   "source": [
    "# preprocess ff++ data\n",
    "preprocess_ffpp(FFPP_SRC, VIDEODF_SRC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8YjT22kUA-MN"
   },
   "source": [
    "### Extract faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_faces_on_video(video_df, source_dir, faces_dir, checkpoint_dir, \n",
    "                           blazeface, num_frames, face_size=224, margin=0.25):\n",
    "    \"\"\"\n",
    "        This function extracts `num_frames` frames from the videos that contain a face.\n",
    "        :param video_df: The DataFrame that contains all information about the datasets. \n",
    "            It has the following columns: [path, name, label, original].\n",
    "        :param source_dir: The parent directory that contains the datasets\n",
    "        :param faces_dir: The directory path to save the extracted faces from the datasets\n",
    "        :param checkpoint_dir: The directory path to save the DataFrame[path, label, video, \n",
    "            original, frame_index, score, detection] of the extracted faces\n",
    "        :param blazeface: A Balazeface object that will be used as the face detector in all \n",
    "            frames\n",
    "        :param num_frames: The number of frames to extract in each video\n",
    "        :param face_size (default = 224) : Each frame extracted will have the size \n",
    "            face_size x face_size\n",
    "        :param margin (default = 0.25) : The offset margin for face detection.\n",
    "    \"\"\"\n",
    "    video_idx, video_df = video_df\n",
    "    faces_checkpoint_path = Path(checkpoint_dir).joinpath(video_df['path'].split('.')[0] + '_faces.pkl')\n",
    "    \n",
    "    try:\n",
    "        s3_resource.Object(bucket_name, str(faces_checkpoint_path)).load()\n",
    "        file_exists = True\n",
    "    except botocore.exceptions.ClientError as e:\n",
    "        if e.response['Error']['Code'] == \"404\":\n",
    "            file_exists = False\n",
    "        else:\n",
    "            raise\n",
    "        \n",
    "    if file_exists: \n",
    "        faces = pickle.loads(s3_resource.Bucket(bucket_name).Object(str(faces_checkpoint_path)).get()['Body'].read())\n",
    "        return faces\n",
    "        \n",
    "    else :\n",
    "        # Télécharger la vidéo depuis S3 dans un buffer\n",
    "        video_path = Path(source_dir).joinpath(video_df['path'])\n",
    "        url = s3_client.generate_presigned_url(ClientMethod='get_object', Params={ 'Bucket': bucket_name, 'Key': str(video_path) })\n",
    "        reader =  cv2.VideoCapture(url)\n",
    "        # Obtenir le nombre de frames de la vidéo\n",
    "        frame_count = int(reader.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        # Obtenir un tableau d'indices de frames uniformément répartis dans la vidéo\n",
    "        frame_idx = np.unique(np.linspace(0, frame_count - 1, num_frames, dtype=int))\n",
    "        # Get the frames choosen\n",
    "        frames, idx = [], 0\n",
    "        # Tant que la vidéo peut être lue\n",
    "        while reader.grab():\n",
    "            if idx in frame_idx:\n",
    "                ret, frame = reader.retrieve()\n",
    "                if not ret or frame is None:\n",
    "                    print(\"Error retrieving frame %d from movie %s\" % (frame_idx, path))\n",
    "                    break\n",
    "                frames.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "            idx += 1\n",
    "        frames = np.stack(frames) # Empiler les frames dans un tableau NumPy\n",
    "        \n",
    "        # Obtenir la largeur et la hauteur cibles pour le modèle BlazeFace\n",
    "        target_w, target_h = blazeface.input_size\n",
    "        # Pour chaque frame, obtenir plusieurs tuiles de taille target_w x target_h\n",
    "        num_frames, height, width, _ = frames.shape\n",
    "            # Définir la taille de chaque tuile en prenant la plus petite valeur parmi height, width et 720\n",
    "        split_size = min(height, width, 720)\n",
    "        x_step = (width - split_size) // 2\n",
    "        y_step = (height - split_size) // 2\n",
    "        num_h = (height - split_size) // y_step + 1 if y_step > 0 else 1\n",
    "        num_w = (width - split_size) // x_step + 1 if x_step > 0 else 1\n",
    "\n",
    "        tiles = np.zeros((num_frames * num_h * num_w, target_h, target_w, 3), \n",
    "                        dtype=np.uint8)\n",
    "        i = 0\n",
    "        for f in range(num_frames):\n",
    "            y = 0\n",
    "            for _ in range(num_h):\n",
    "                x = 0\n",
    "                for __ in range(num_w):\n",
    "                    # Découper une tuile à partir de la frame actuelle\n",
    "                    crop = frames[f, y:y + split_size, x:x + split_size, :]\n",
    "                    # Redimensionner la tuile à la taille cible en utilisant une interpolation par aire\n",
    "                    tiles[i] = cv2.resize(crop, (target_w, target_h), \n",
    "                                        interpolation=cv2.INTER_AREA)\n",
    "                    x += x_step\n",
    "                    i += 1\n",
    "                y += y_step\n",
    "\n",
    "        # Run the face detector. The result is a list of PyTorch tensors\n",
    "        detections = blazeface.predict_on_batch(tiles, apply_nms=False)\n",
    "        # Convert the detections from 128x128 back to the original frame size\n",
    "        for i in range(len(detections)):\n",
    "            # ymin, xmin, ymax, xmax\n",
    "            for k in range(2):\n",
    "                detections[i][:, k * 2] = (detections[i][:, k * 2] * target_h) * split_size / target_h\n",
    "                detections[i][:, k * 2 + 1] = (detections[i][:, k * 2 + 1] * target_w) * split_size / target_w\n",
    "\n",
    "        # Because we have several tiles for each frame, combine the predictions from these tiles.\n",
    "        combined_detections = []\n",
    "        i = 0\n",
    "        for f in range(num_frames):\n",
    "            detections_for_frame = []\n",
    "            y = 0\n",
    "            for _ in range(num_h):\n",
    "                x = 0\n",
    "                for __ in range(num_w):\n",
    "                    # Adjust the coordinates based on the split positions.\n",
    "                    if detections[i].shape[0] > 0:\n",
    "                        for k in range(2):\n",
    "                            detections[i][:, k * 2] += y\n",
    "                            detections[i][:, k * 2 + 1] += x\n",
    "                    \n",
    "                    detections_for_frame.append(detections[i])\n",
    "                    x += x_step\n",
    "                    i += 1\n",
    "                y += y_step\n",
    "                \n",
    "            combined_detections.append(torch.cat(detections_for_frame))\n",
    "        if len(combined_detections) == 0:\n",
    "            return None\n",
    "        detections = blazeface.nms(combined_detections)\n",
    "        # Crop the faces out of the original frame.\n",
    "        faces = []\n",
    "        for i in range(len(detections)):\n",
    "            offset = torch.round(margin * (detections[i][:, 2] - detections[i][:, 0])) # margin 0.2\n",
    "            detections[i][:, 0] = torch.clamp(detections[i][:, 0] - offset * 2, min=0)  # ymin\n",
    "            detections[i][:, 1] = torch.clamp(detections[i][:, 1] - offset, min=0)  # xmin\n",
    "            detections[i][:, 2] = torch.clamp(detections[i][:, 2] + offset, max=height)  # ymax\n",
    "            detections[i][:, 3] = torch.clamp(detections[i][:, 3] + offset, max=width)  # xmax\n",
    "            \n",
    "            # Get the first best scored face\n",
    "            score, face, detection = 0, None, None\n",
    "            for j in range(len(detections[i])):\n",
    "                if score < detections[i][j][16].cpu():\n",
    "                    detection = detections[i][j].cpu()\n",
    "                    ymin, xmin, ymax, xmax = detection[:4].cpu().numpy().astype(int)\n",
    "                    face = frames[i][ymin:ymax, xmin:xmax, :]\n",
    "                    score = detection[16]\n",
    "                    break\n",
    "            if face is not None:\n",
    "                image = Image.fromarray(face)\n",
    "                # Crop the image to face_size x face_size\n",
    "                top, left, bottom, right = detection[:4].cpu().numpy().astype(int)\n",
    "                x_ctr = (left + right) // 2\n",
    "                y_ctr = (top + bottom) // 2\n",
    "                new_top = max(y_ctr - face_size // 2, 0)\n",
    "                new_bottom = min(new_top + face_size, height)\n",
    "                new_left = max(x_ctr - face_size // 2, 0)\n",
    "                new_right = min(new_left + face_size, width)\n",
    "                image.crop([new_left, new_top, new_right, new_bottom])\n",
    "                # Save image\n",
    "                face_path = Path(faces_dir).joinpath(video_df['path']).joinpath('frame_{}.jpg'.format(frame_idx[i]))\n",
    "                buf = io.BytesIO()\n",
    "                image.save(buf, format=\"jpeg\")\n",
    "                object = bucket.Object(str(face_path))\n",
    "                object.put(Body=buf.getvalue())\n",
    "                faces.append({\n",
    "                    'path': str(Path(video_df['path']).joinpath('frame_{}.jpg'.format(frame_idx[i]))),\n",
    "                    'label': video_df['label'],\n",
    "                    'video': video_idx,\n",
    "                    'original': video_df['original'],\n",
    "                    'frame_index': frame_idx[i],\n",
    "                    'score': float(score.numpy()),\n",
    "                    'detection': detection[:4].cpu().numpy().astype(int)\n",
    "                })\n",
    "            # Save checkpoint\n",
    "            buf = io.BytesIO()\n",
    "            pd.DataFrame(faces).to_pickle(buf)\n",
    "            buf.seek(0)\n",
    "            bucket.upload_fileobj(buf, str(faces_checkpoint_path))\n",
    "\n",
    "        return faces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "qIy8YsWJA-MU",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_faces(source_dir, videos_df, faces_dir, faces_df, checkpoint_dir, \n",
    "                  frames_per_video=15, batch_size=32, face_size=224, thread_num=7):\n",
    "    \"\"\"\n",
    "        This function extracts all frames from the dataset that contain a face.\n",
    "        :param source_dir: The parent directory that contains the datasets\n",
    "        :param videos_df: The path of the DataFrame containing all information \n",
    "            about the videos in the dataset.\n",
    "        :param faces_dir: The directory path to save the extracted faces from the datasets\n",
    "        :param faces_df: The path to save the DataFrame containing all informations \n",
    "            about the extracted faces\n",
    "        :param checkpoint_dir: The directory path to save the DataFrame[path, label, \n",
    "            video, original, frame_index, score, detection] of the extracted faces\n",
    "        :param frames_per_video (default = 15): The number of frames to extract in each video\n",
    "        :param batch_size (default = 16): The batch size of videos to process together\n",
    "        :param face_size (default = 224) : Each frame extracted will have the size face_size x face_size\n",
    "        :thread_num (default = 4): The number of threads to be used during the extraction\n",
    "    \"\"\"\n",
    "    # On vérifie si ffpp_faces.pkl existe\n",
    "    try:\n",
    "        s3_resource.Object(bucket_name, faces_df).load()\n",
    "        file_exists = True\n",
    "    except botocore.exceptions.ClientError as e:\n",
    "        if e.response['Error']['Code'] == \"404\":\n",
    "            file_exists = False\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "    # Charger le fichier de checkpoint depuis S3 s'il existe\n",
    "    if file_exists:\n",
    "        df_faces = pickle.loads(s3_resource.Bucket(bucket_name).Object(faces_df).get()['Body'].read())\n",
    "        print('We got {} faces'.format(len(df_faces)))\n",
    "        print('Faces DataFrame Loaded')\n",
    "        return df_faces\n",
    "    \n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    print('Loading video DataFrame')\n",
    "    df_videos = pickle.loads(s3_resource.Bucket(bucket_name).Object(videos_df).get()['Body'].read())\n",
    "    \n",
    "    print('Loading Blazeface model')\n",
    "    blazeface_net = BlazeFace().to(device)\n",
    "    blazeface_net.load_weights(io.BytesIO(s3_resource.Bucket(bucket_name).Object(BLAZEFACE_WEIGHTS).get()['Body'].read()))\n",
    "    blazeface_net.load_anchors(io.BytesIO(s3_resource.Bucket(bucket_name).Object(BLAZEFACE_ANCHORS).get()['Body'].read()))    \n",
    "    blazeface_net.min_score_thresh = 0.8\n",
    "    \n",
    "    ## Face extraction\n",
    "    with ThreadPoolExecutor(thread_num) as pool:\n",
    "        for batch_idx0 in tqdm(np.arange(start=0, stop=len(df_videos), step=batch_size),\n",
    "                               desc='Extracting faces'):\n",
    "            list(pool.map(partial(extract_faces_on_video,\n",
    "                          source_dir=source_dir,\n",
    "                          faces_dir=faces_dir,\n",
    "                          checkpoint_dir=checkpoint_dir,\n",
    "                          blazeface=blazeface_net,\n",
    "                          num_frames=frames_per_video,\n",
    "                          face_size=face_size,\n",
    "                          ),\n",
    "                          df_videos.iloc[batch_idx0:batch_idx0 + batch_size].iterrows()))\n",
    "    \n",
    "    faces_dataset = []\n",
    "    for _, df in tqdm(df_videos.iterrows(), total=len(df_videos), desc='Collecting faces results'):\n",
    "        face_checkpoint = Path(checkpoint_dir).joinpath(df['path'].split('.')[0] + '_faces.pkl')\n",
    "        try:\n",
    "            s3_resource.Object(bucket_name, str(face_checkpoint)).load()\n",
    "            face_checkpoint_exists = True\n",
    "        except botocore.exceptions.ClientError as e:\n",
    "            if e.response['Error']['Code'] == \"404\":\n",
    "                file_exists = False\n",
    "            else:\n",
    "                raise\n",
    "        if face_checkpoint_exists:\n",
    "            df_face = pickle.loads(s3_resource.Bucket(bucket_name).Object(str(face_checkpoint)).get()['Body'].read())\n",
    "            faces_dataset.append(df_face)\n",
    "        else:\n",
    "            print(f'Checkpoint file {face_checkpoint} does not exist')\n",
    "            \n",
    "    df_faces = pd.concat(faces_dataset, axis=0)\n",
    "    buf = io.BytesIO()\n",
    "    df_faces.to_pickle(buf)\n",
    "    buf.seek(0)\n",
    "    bucket.upload_fileobj(buf, faces_df)\n",
    "    print('We got {} faces'.format(len(df_faces)))\n",
    "    print('Completed!')\n",
    "    return df_faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "Wy0Y5AwtA-Mb",
    "outputId": "9767bcf6-8915-4771-9682-4f7f5fad372a",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We got 330 faces\n",
      "Faces DataFrame Loaded\n"
     ]
    }
   ],
   "source": [
    "# Run extraction\n",
    "df_faces = extract_faces(FFPP_SRC, VIDEODF_SRC, FACES_DST, FACESDF_DST,  CHECKPOINT_DST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mIzeeGplA-NQ"
   },
   "source": [
    "## Dataset processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "6_y-9WSIA-NR",
    "tags": []
   },
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hb3FfzYeA-NU"
   },
   "source": [
    "### Split dataset on original video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "rLAoN8D1A-NU",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_splits(df_faces, train_ratio=0.7, val_ratio=0.15):\n",
    "    \"\"\"\n",
    "    Splitting the dataset into three subsets : train, validation and test.\n",
    "    \"\"\"\n",
    "    random_original_videos = np.random.permutation(df_faces[(df_faces['label'] == 0)]['video'].unique())\n",
    "    train_num = int(len(random_original_videos) * train_ratio)\n",
    "    val_num = int(len(random_original_videos) * val_ratio)\n",
    "    train_original = random_original_videos[:train_num]\n",
    "    val_original = random_original_videos[train_num: train_num + val_num]\n",
    "    test_original = random_original_videos[train_num + val_num:]\n",
    "    \n",
    "    df_train = pd.concat([df_faces[df_faces['original'].isin(train_original)], \n",
    "                          df_faces[df_faces['video'].isin(train_original)]], ignore_index=True)\n",
    "    df_val = pd.concat([df_faces[df_faces['original'].isin(val_original)], \n",
    "                        df_faces[df_faces['video'].isin(val_original)]], ignore_index=True)\n",
    "    df_test = pd.concat([df_faces[df_faces['original'].isin(test_original)], \n",
    "                         df_faces[df_faces['video'].isin(test_original)]], ignore_index=True)\n",
    "    \n",
    "    return df_train, df_val, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "dtLSibzXCXIA",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_faces = pickle.loads(s3_resource.Bucket(bucket_name).Object(FACESDF_DST).get()['Body'].read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "2FdyPSuBChgk",
    "outputId": "c538a570-93ac-40f5-f9c3-c2985a55d974",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>label</th>\n",
       "      <th>video</th>\n",
       "      <th>original</th>\n",
       "      <th>frame_index</th>\n",
       "      <th>score</th>\n",
       "      <th>detection</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>manipulated_sequences/Deepfakes/c23/videos/038...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0.898076</td>\n",
       "      <td>[4, 427, 263, 649]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>manipulated_sequences/Deepfakes/c23/videos/038...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>21</td>\n",
       "      <td>0.943305</td>\n",
       "      <td>[5, 432, 258, 650]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>manipulated_sequences/Deepfakes/c23/videos/038...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>43</td>\n",
       "      <td>0.935171</td>\n",
       "      <td>[6, 433, 272, 661]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>manipulated_sequences/Deepfakes/c23/videos/038...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>65</td>\n",
       "      <td>0.939342</td>\n",
       "      <td>[5, 443, 271, 671]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>manipulated_sequences/Deepfakes/c23/videos/038...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>86</td>\n",
       "      <td>0.924075</td>\n",
       "      <td>[4, 443, 254, 657]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path  label  video  original  \\\n",
       "0  manipulated_sequences/Deepfakes/c23/videos/038...      1      0        11   \n",
       "1  manipulated_sequences/Deepfakes/c23/videos/038...      1      0        11   \n",
       "2  manipulated_sequences/Deepfakes/c23/videos/038...      1      0        11   \n",
       "3  manipulated_sequences/Deepfakes/c23/videos/038...      1      0        11   \n",
       "4  manipulated_sequences/Deepfakes/c23/videos/038...      1      0        11   \n",
       "\n",
       "   frame_index     score           detection  \n",
       "0            0  0.898076  [4, 427, 263, 649]  \n",
       "1           21  0.943305  [5, 432, 258, 650]  \n",
       "2           43  0.935171  [6, 433, 272, 661]  \n",
       "3           65  0.939342  [5, 443, 271, 671]  \n",
       "4           86  0.924075  [4, 443, 254, 657]  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_faces.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "paths = df_faces['path']\n",
    "dir_paths = [os.path.dirname(p) for p in paths]\n",
    "df_faces['dir_path'] = dir_paths\n",
    "df_faces['s3_path'] = df_faces['dir_path'].map(lambda x: f's3://{bucket_name}/{FACES_DST}/{x}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "KLy4qAuQA-NZ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train, df_val, df_test = make_splits(df_faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "id": "AlpQAHFPA-Nc",
    "outputId": "dfdbc799-3d9e-4b5a-e550-53630693cfc6",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dataset : 330\n",
      "Total train : 210\n",
      "Total val : 30\n",
      "Total test : 90\n"
     ]
    }
   ],
   "source": [
    "r_train, _ = df_train.shape\n",
    "r_val, _ = df_val.shape\n",
    "r_test, _ = df_test.shape\n",
    "\n",
    "print('Total dataset : {}'.format(r_train+r_val+r_test))\n",
    "print('Total train : {}'.format(r_train))\n",
    "print('Total val : {}'.format(r_val))\n",
    "print('Total test : {}'.format(r_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy train, valid and test to specific folder \n",
    "# copy_preprocessed_data('train', df_train['path'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4pVM8ty6A-Nf"
   },
   "source": [
    "### Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "RR0i7TC5A-Nf",
    "tags": []
   },
   "outputs": [],
   "source": [
    "pre_trained_mean, pre_trained_std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomAffine(degrees=40, scale=(.9, 1.1), shear=0),\n",
    "    transforms.RandomPerspective(distortion_scale=0.2),\n",
    "    transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomErasing(scale=(0.02, 0.16), ratio=(0.3, 1.6)),\n",
    "    transforms.Normalize(mean=pre_trained_mean, std=pre_trained_std)\n",
    "])\n",
    "\n",
    "val_tranform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=pre_trained_mean, std=pre_trained_std)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f8iG1vBAA-Ni"
   },
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "_YB92wiiA-Nj",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FFPPDataset(Dataset):\n",
    "    def __init__(self, df_faces, faces_dir=FACES_DST, transform=None):\n",
    "        super().__init__()\n",
    "        self.faces_dir = Path(faces_dir)\n",
    "        self.data, self.targets = df_faces['path'], df_faces['label']\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        img_path, target = self.data[index], self.targets[index]\n",
    "        target = np.array([target,]).astype(np.float32)\n",
    "        \n",
    "        file_stream = bucket.Object(str(self.faces_dir.joinpath(img_path))).get()['Body']\n",
    "        img = Image.open(file_stream)\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return img, target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import IterableDataset\n",
    "from torchdata.datapipes.iter import IterableWrapper, S3FileLoader\n",
    "\n",
    "class S3FFPPDataset(IterableDataset):\n",
    "    def __init__(self, df, faces_dir=FACES_DST, shuffle_urls=False, transform=None):\n",
    "        super().__init__()\n",
    "        self.data = df\n",
    "        self.paths = IterableWrapper(df['s3_path'].unique().tolist()).list_files_by_s3()\n",
    "        self.targets = IterableWrapper(df['label'])\n",
    "        if shuffle_urls:\n",
    "            self.sharded_s3_urls = self.paths.shuffle().sharding_filter()\n",
    "            self.s3_files = S3FileLoader(self.sharded_s3_urls)\n",
    "        else:\n",
    "            self.s3_files = S3FileLoader(self.paths)\n",
    "        self.transform = transform\n",
    "    \n",
    "    def data_generator(self):\n",
    "        try:\n",
    "            while True:\n",
    "                url, stream = next(self.s3_files_iterator)\n",
    "                target = next(self.targets_iterator)\n",
    "                \n",
    "                target = np.array([target,]).astype(np.float32)\n",
    "                img = Image.open(stream)\n",
    "                \n",
    "                if self.transform is not None:\n",
    "                    img = self.transform(img)\n",
    "                yield img, target\n",
    "\n",
    "        except StopIteration:\n",
    "            return\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.s3_files_iterator = iter(self.s3_files)\n",
    "        self.targets_iterator = iter(self.targets)\n",
    "        return self.data_generator()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rxBdI_boA-Nm",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_dataset = S3FFPPDataset(df_train, shuffle_urls=True, transform=train_transform)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=32, \n",
    "#                           num_workers=0)\n",
    "\n",
    "# valid_dataset = S3FFPPDataset(df_val, transform=val_tranform)\n",
    "# val_loader = DataLoader(valid_dataset, batch_size=32, num_workers=0)\n",
    "\n",
    "# test_dataset = S3FFPPDataset(df_test, transform=val_tranform)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=32, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = FFPPDataset(df_train, transform=train_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True,\n",
    "                          num_workers=4)\n",
    "\n",
    "valid_dataset = FFPPDataset(df_val, transform=val_tranform)\n",
    "val_loader = DataLoader(valid_dataset, batch_size=32, num_workers=4)\n",
    "\n",
    "test_dataset = FFPPDataset(df_test, transform=val_tranform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_train = next(iter(train_loader))\n",
    "batch_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y7swTRXvMbld"
   },
   "source": [
    "## Train EfficientNet\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BVLsLQ3N1Ha5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "# number of epochs to train for\n",
    "num_epochs = 10\n",
    "\n",
    "# learning rate\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# loss_function\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8NTgHso71zkG",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(net, model_name):\n",
    "    base_path = 'model_checkpoint'\n",
    "    model_path = os.path.join(base_path, \n",
    "                            f'models/{model_name}' + '/best_epoch_{epoch}.ckpt')\n",
    "    Path(model_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    logs_file = os.path.join(base_path, f'logs/{model_name}/log.tsv')\n",
    "    Path(logs_file).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    callbacks = [\n",
    "        # Save the weights in a new file when the current model is better than all \n",
    "        # previous models.\n",
    "        ModelCheckpoint(model_path, monitor='val_bin_acc', mode='max', \n",
    "                        save_best_only=True, restore_best=True, verbose=True, \n",
    "                        temporary_filename=model_name + '_best_epoch.ckpt.tmp'),\n",
    "\n",
    "        # Save the losses and accuracies for each epoch in a TSV.\n",
    "        CSVLogger(logs_file, separator='\\t'),\n",
    "    ]\n",
    "\n",
    "    params = (p for p in net.parameters() if p.requires_grad)\n",
    "    optimizer = optim.Adam(params, lr=learning_rate, weight_decay=0.000001)\n",
    "\n",
    "    model = Model(net, optimizer, criterion, batch_metrics=['bin_acc'])\n",
    "    model.to(device)\n",
    "\n",
    "    if Path(logs_file).exists():\n",
    "        logs = pd.read_csv(logs_file, sep='\\t')\n",
    "        epochs = list(logs['epoch'])\n",
    "        if len(epochs) != 0:\n",
    "            best_epoch_idx = logs['val_bin_acc'].idxmax()\n",
    "            best_epoch = int(logs.loc[best_epoch_idx]['epoch'])\n",
    "            model.load_weights(model_path.format(epoch=best_epoch))\n",
    "            if num_epochs not in epochs:\n",
    "                # Train\n",
    "                model.fit_generator(train_loader, val_loader, epochs=num_epochs, initial_epoch=epochs[-1], callbacks=callbacks)\n",
    "        else:\n",
    "            model.fit_generator(train_loader, val_loader, epochs=num_epochs, callbacks=callbacks)\n",
    "    else:\n",
    "        model.fit_generator(train_loader, val_loader, epochs=num_epochs, callbacks=callbacks)\n",
    "    \n",
    "    logs = pd.read_csv(logs_file, sep='\\t')\n",
    "    \n",
    "    best_epoch_idx = logs['val_bin_acc'].idxmax()\n",
    "    best_epoch = int(logs.loc[best_epoch_idx]['epoch'])\n",
    "    print(\"Best epoch: %d\" % best_epoch)\n",
    "    \n",
    "    metrics = ['loss', 'val_loss']\n",
    "    plt.plot(logs['epoch'], logs[metrics])\n",
    "    plt.legend(metrics)\n",
    "    plt.title('Loss')\n",
    "    plt.show()\n",
    "\n",
    "    metrics = ['bin_acc', 'val_bin_acc']\n",
    "    plt.plot(logs['epoch'], logs[metrics])\n",
    "    plt.legend(metrics)\n",
    "    plt.title('Accuracy')\n",
    "    plt.show()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nKbzkBCb-vQ3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test(model, test_loader):\n",
    "    test_loss, test_acc = model.evaluate_generator(test_loader)\n",
    "    print('Test:\\n\\tLoss: {}\\n\\tAccuracy: {}'.format(test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oc8EM1TS1FyQ"
   },
   "source": [
    "### Freezant tous les paramètres de convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 630
    },
    "id": "gkzbm95FFODY",
    "outputId": "5cdc683b-a6c8-45e5-c17b-6ca6ac962b2e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "efficientnet_a = EfficientNet.from_pretrained('efficientnet-b4')\n",
    "efficientnet_a._fc = nn.Linear(efficientnet_a._conv_head.out_channels, 1)\n",
    "\n",
    "for name, param in efficientnet_a.named_parameters():\n",
    "    if not name.startswith('_fc'):\n",
    "        param.requires_grad = False\n",
    "\n",
    "model_a = train(efficientnet_a, 'efficientnet_a')\n",
    "test(model_a, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T5_FQ-nrQAYz"
   },
   "source": [
    "### En réentraînant quelques blocs\n",
    "\n",
    "Réentraîner les 8 derniers blocs avec les deux dernières couches (couche de convolution et classificateur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "070eb8acf3da4a90b422357a557c02de",
      "a97360270ec9405185501d1d5d1dcb49",
      "be1a54c2a1584186aad0ccb38ce488e6",
      "d1e20ce9d6334d43bdca7187103ba40b",
      "1b7d182d4e714faea9e3f810e5432408",
      "14f55ce43863405ab8adbe64cbbc86fb",
      "e58c63dc8ef649afb481b3f92aafd1fd",
      "d3c6935c60c84e2a857ac0040d837288"
     ]
    },
    "id": "mGvhRsFfULPj",
    "outputId": "49589859-6fe6-47f7-ee04-9b75ffb02010"
   },
   "outputs": [],
   "source": [
    "efficientnet_b = EfficientNet.from_pretrained('efficientnet-b4')\n",
    "efficientnet_b._fc = nn.Linear(efficientnet_b._conv_head.out_channels, 1)\n",
    "\n",
    "names = [name for name, _ in list(efficientnet_b.named_parameters())[-109:]]\n",
    "for name, param in efficientnet_b.named_parameters():\n",
    "    if name not in names:\n",
    "        param.requires_grad = False\n",
    "\n",
    "model_b = train(efficientnet_b, 'efficientnet_b')\n",
    "test(model_b, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tq3kPtqzmEsJ"
   },
   "source": [
    "Réentraîner les 16 derniers blocs avec les deux dernières couches (couche de convolution et classificateur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "69494fc6ec25404aafdb3039e766ced3",
      "fd12c8734c9e4366bb43665fa565d57e",
      "556ecf641eb744929281c8b94c929da6",
      "2477092084fe428fb571e8f9968ad796",
      "d5d44796e5014f48a389ab9717f1d9fa",
      "5438b806f5c248a6ada5935522b7d4bb",
      "4f780573f05d4d1a9668ca7f80921bf2",
      "951256577b0c45dea9278149903d5343"
     ]
    },
    "id": "e2phbJ577fhQ",
    "outputId": "cece2b2a-04c6-4c60-d4d4-38266a0a6f02"
   },
   "outputs": [],
   "source": [
    "efficientnet_c = EfficientNet.from_pretrained('efficientnet-b4')\n",
    "efficientnet_c._fc = nn.Linear(efficientnet_c._conv_head.out_channels, 1)\n",
    "\n",
    "names = [name for name, _ in list(efficientnet_c.named_parameters())[-213:]]\n",
    "for name, param in efficientnet_c.named_parameters():\n",
    "    if name not in names:\n",
    "        param.requires_grad = False\n",
    "\n",
    "model_c = train(efficientnet_c, 'efficientnet_c')\n",
    "test(model_c, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_tQJWTsnhWtk"
   },
   "source": [
    "## Mécanisme d'attention\n",
    "\n",
    "Ajouter un mécanisme d'attention visuelle : Spatial Transformer Network (Applique une transformation spatiale sur les\n",
    "feature maps, lors du forward pass) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WhHp-pz8qX1X"
   },
   "outputs": [],
   "source": [
    "class EfficientNetAtt(EfficientNet):\n",
    "    def __init__(self, blocks_args=None, global_params=None):\n",
    "        super().__init__(blocks_args, global_params)\n",
    "        \n",
    "    def init_spatial_transformer(self):\n",
    "        self.block_idx = 20\n",
    "        \n",
    "        # Spatial transformer localization-network\n",
    "        self.localization = nn.Sequential(\n",
    "            nn.Conv2d(160, 160, kernel_size=5),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(160, 80, kernel_size=3),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        \n",
    "        # Regressor for the 3 * 2 affine matrix\n",
    "        self.fc_loc = nn.Sequential(\n",
    "            nn.Linear(80 * 1 * 1, 32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(32, 3 * 2)\n",
    "        )\n",
    "        \n",
    "        # Initialize the weights/bias with identity transformation\n",
    "        self.fc_loc[2].weight.data.zero_()\n",
    "        self.fc_loc[2].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], \n",
    "                                                    dtype=torch.float))\n",
    "    \n",
    "    def stn(self, inputs):\n",
    "        inputs = inputs.clone()\n",
    "        xs = self.localization(inputs)\n",
    "        xs = xs.view(-1, 80 * 1 * 1)\n",
    "        theta = self.fc_loc(xs)\n",
    "        theta = theta.view(-1, 2, 3)\n",
    "\n",
    "        grid = F.affine_grid(theta, inputs.size(), align_corners=False)\n",
    "        x = F.grid_sample(inputs, grid, align_corners=False)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def extract_features(self, inputs):\n",
    "        \"\"\" Returns output of the final convolution layer \"\"\"\n",
    "\n",
    "        # Stem\n",
    "        x = self._swish(self._bn0(self._conv_stem(inputs)))\n",
    "\n",
    "        # Blocks\n",
    "        for idx, block in enumerate(self._blocks):\n",
    "            drop_connect_rate = self._global_params.drop_connect_rate\n",
    "            if drop_connect_rate:\n",
    "                drop_connect_rate *= float(idx) / len(self._blocks)\n",
    "            x = block(x, drop_connect_rate=drop_connect_rate)\n",
    "            if idx == self.block_idx:\n",
    "                x *= self.stn(x)\n",
    "\n",
    "        # Head\n",
    "        x = self._swish(self._bn1(self._conv_head(x)))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "b7ac4ff7d8004461b9ea3b5d83cee0b1",
      "f54f6addc4494529a4f8b7274aa98117",
      "52e77c570ca1490d9912a0ddc085e810",
      "35ed6513b0024b709df00f26e9b3cea4",
      "e4aeeb499d5a443baa963a3ed489d7ae",
      "0085da15665a4a46a08eb67d95457101",
      "182db4cbdac549f8840138a5dab43d58",
      "5e4d16b0351b4c1d85b9b883c2a65a8a"
     ]
    },
    "id": "uCefQSQCiKKa",
    "outputId": "69c2662a-bdb4-48e0-e448-8f87f91ff34c"
   },
   "outputs": [],
   "source": [
    "efficientnet_d = EfficientNetAtt.from_pretrained('efficientnet-b4')\n",
    "efficientnet_d._fc = nn.Linear(efficientnet_d._conv_head.out_channels, 1)\n",
    "efficientnet_d.init_spatial_transformer()\n",
    "\n",
    "names = [name for name, _ in list(efficientnet_d.named_parameters())[-221:]]\n",
    "for name, param in efficientnet_d.named_parameters():\n",
    "    if name not in names:\n",
    "        param.requires_grad = False\n",
    "\n",
    "model_d = train(efficientnet_d, 'efficientnet_d')\n",
    "test(model_d, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Base Python 2.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-base-python-38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "3bc196b68f548d1b4a7b23b62dc36c96d4883bf50034a10858aedfd454af5fac"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0085da15665a4a46a08eb67d95457101": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "070eb8acf3da4a90b422357a557c02de": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_be1a54c2a1584186aad0ccb38ce488e6",
       "IPY_MODEL_d1e20ce9d6334d43bdca7187103ba40b"
      ],
      "layout": "IPY_MODEL_a97360270ec9405185501d1d5d1dcb49"
     }
    },
    "14f55ce43863405ab8adbe64cbbc86fb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "182db4cbdac549f8840138a5dab43d58": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1b7d182d4e714faea9e3f810e5432408": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "2477092084fe428fb571e8f9968ad796": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_951256577b0c45dea9278149903d5343",
      "placeholder": "​",
      "style": "IPY_MODEL_4f780573f05d4d1a9668ca7f80921bf2",
      "value": " 74.4M/74.4M [00:05&lt;00:00, 13.7MB/s]"
     }
    },
    "35ed6513b0024b709df00f26e9b3cea4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5e4d16b0351b4c1d85b9b883c2a65a8a",
      "placeholder": "​",
      "style": "IPY_MODEL_182db4cbdac549f8840138a5dab43d58",
      "value": " 74.4M/74.4M [00:05&lt;00:00, 13.2MB/s]"
     }
    },
    "4f780573f05d4d1a9668ca7f80921bf2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "52e77c570ca1490d9912a0ddc085e810": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0085da15665a4a46a08eb67d95457101",
      "max": 77999237,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e4aeeb499d5a443baa963a3ed489d7ae",
      "value": 77999237
     }
    },
    "5438b806f5c248a6ada5935522b7d4bb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "556ecf641eb744929281c8b94c929da6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5438b806f5c248a6ada5935522b7d4bb",
      "max": 77999237,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d5d44796e5014f48a389ab9717f1d9fa",
      "value": 77999237
     }
    },
    "5e4d16b0351b4c1d85b9b883c2a65a8a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "69494fc6ec25404aafdb3039e766ced3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_556ecf641eb744929281c8b94c929da6",
       "IPY_MODEL_2477092084fe428fb571e8f9968ad796"
      ],
      "layout": "IPY_MODEL_fd12c8734c9e4366bb43665fa565d57e"
     }
    },
    "951256577b0c45dea9278149903d5343": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a97360270ec9405185501d1d5d1dcb49": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b7ac4ff7d8004461b9ea3b5d83cee0b1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_52e77c570ca1490d9912a0ddc085e810",
       "IPY_MODEL_35ed6513b0024b709df00f26e9b3cea4"
      ],
      "layout": "IPY_MODEL_f54f6addc4494529a4f8b7274aa98117"
     }
    },
    "be1a54c2a1584186aad0ccb38ce488e6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_14f55ce43863405ab8adbe64cbbc86fb",
      "max": 77999237,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1b7d182d4e714faea9e3f810e5432408",
      "value": 77999237
     }
    },
    "d1e20ce9d6334d43bdca7187103ba40b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d3c6935c60c84e2a857ac0040d837288",
      "placeholder": "​",
      "style": "IPY_MODEL_e58c63dc8ef649afb481b3f92aafd1fd",
      "value": " 74.4M/74.4M [00:00&lt;00:00, 102MB/s]"
     }
    },
    "d3c6935c60c84e2a857ac0040d837288": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d5d44796e5014f48a389ab9717f1d9fa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "e4aeeb499d5a443baa963a3ed489d7ae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "e58c63dc8ef649afb481b3f92aafd1fd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f54f6addc4494529a4f8b7274aa98117": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fd12c8734c9e4366bb43665fa565d57e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
